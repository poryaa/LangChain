{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c662778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-openai langchain-community\n",
    "#pip install langchain-text-splitters langchain-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9166a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e1c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: anyio in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/paminidigehsara/miniconda3/envs/LangChain/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74961b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='...blue!' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-08-26T06:46:33.999008946Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4238953541, 'load_duration': 3523579339, 'prompt_eval_count': 14, 'prompt_eval_duration': 454690673, 'eval_count': 4, 'eval_duration': 259684517, 'model_name': 'llama3.1'} id='run--60e02bdc-30a9-4c42-8be1-9e4c2995bd36-0' usage_metadata={'input_tokens': 14, 'output_tokens': 4, 'total_tokens': 18}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "response = model.invoke(\"The sky is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ebba32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Paris!!!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-25T11:14:15.607751167Z', 'done': True, 'done_reason': 'stop', 'total_duration': 639231776, 'load_duration': 26177942, 'prompt_eval_count': 35, 'prompt_eval_duration': 445738077, 'eval_count': 3, 'eval_duration': 166496181, 'model_name': 'llama3.1'}, id='run--f700cb3d-fb6a-4b46-bb55-731c2d7eb0ba-0', usage_metadata={'input_tokens': 35, 'output_tokens': 3, 'total_tokens': 38})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ['system', '''you are a helpful assistant that responds to questions with three exclamation marks.'''\n",
    "    'user',\"what is the capital of France?\"]\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9036783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face (through its `transformers` library), OpenAI (through its `openai` library), and Cohere (through its `cohere` library) are the model providers mentioned as offering LLMs.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-08-25T11:24:52.104636149Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9696440840, 'load_duration': 1969566613, 'prompt_eval_count': 137, 'prompt_eval_duration': 3798133137, 'eval_count': 47, 'eval_duration': 3927924035, 'model_name': 'llama3.1'} id='run--487ace19-10b5-4be2-80a3-f915f64288f0-0' usage_metadata={'input_tokens': 137, 'output_tokens': 47, 'total_tokens': 184}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template =PromptTemplate.from_template(\"\"\"Answer the question based on the context below./\n",
    "                                        If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs)./\"\n",
    "        \" These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\"})\n",
    "response =model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e43717da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the context, three providers are mentioned that offer Large Language Models (LLMs):\\n\\n1. Hugging Face (through their `transformers` library)\\n2. OpenAI (through their `openai` library)\\n3. Cohere (through their `cohere` library)', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-25T11:34:12.765013016Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10718323065, 'load_duration': 2023059240, 'prompt_eval_count': 135, 'prompt_eval_duration': 3732852441, 'eval_count': 59, 'eval_duration': 4961079238, 'model_name': 'llama3.1'}, id='run--412bb645-9b84-4593-bb23-c394bdf3f2a8-0', usage_metadata={'input_tokens': 135, 'output_tokens': 59, 'total_tokens': 194})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''Answer the question based on the context below. if the question cannot be answered using the information provided, answer with \"\"I dont Know.'''),\n",
    "    (\"human\", \"Context:{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\"}\n",
    ")\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f600807e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='both weigh the same!', justification='One pound is one pound regardless of what you put in that pound. Therefore both a pound of bricks and a pound of feathers would weigh one pound.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    justification: str\n",
    "    '''justification for the answer'''\n",
    "llm = ChatOllama(model='llama3.1')\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound of feathers\"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eef202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Several models have been trained to produce large language models (LLMs). Here are some of them:\\n\\n1. **Bloom**: Developed by Meta AI, Bloom is an open-source, highly scalable LLM that can process large amounts of text.\\n2. **Longformer**: This model was developed by researchers at Google and is designed for long-range dependencies in text data.\\n3. **RoBERTa**: Built on top of BERT, RoBERTa is a widely used LLM that has been fine-tuned for various NLP tasks such as question-answering and sentiment analysis.\\n4. **T5 (Text-to-Text Transfer Tranformer)**: Developed by Google, T5 is a highly versatile model that can be fine-tuned for multiple tasks, including text classification, entity recognition, and machine translation.\\n5. **LLaMA**: This LLM was developed by Meta AI and is designed to handle long-range dependencies in text data.\\n\\nThese models are widely used in various applications such as chatbots, language translation, text summarization, and more.\\n\\nWould you like me to explain any of these models in more detail or provide examples of their use cases?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-25T12:24:25.388047385Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23219606550, 'load_duration': 2019424027, 'prompt_eval_count': 30, 'prompt_eval_duration': 890305176, 'eval_count': 238, 'eval_duration': 20309102522, 'model_name': 'llama3.1'}, id='run--0034c338-511a-45e5-9c41-5388f6bd6fcd-0', usage_metadata={'input_tokens': 30, 'output_tokens': 238, 'total_tokens': 268})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'you are a helpful assistant'),\n",
    "    ('human, {question}')\n",
    "])\n",
    "\n",
    "model =ChatOllama(model='llama3.1')\n",
    "\n",
    "chatbot = template | model \n",
    "\n",
    "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a81af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
