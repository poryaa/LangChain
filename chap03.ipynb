{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a958e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_text)\n",
    "#splitted_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b220854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# Load the document, split it with chunks\n",
    "text = TextLoader(\"./test.txt\").load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150,chunk_overlap=10)\n",
    "splitted_text = splitter.split_documents(text)\n",
    "\n",
    "# Embed each chunk and insert it into the vector store\n",
    "embedding_model = OllamaEmbeddings(model =\"llama3.1\")\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "db = PGVector.from_documents(splitted_text, embedding=embedding_model, connection=connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edb48332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c4ca181a-ae8c-4408-94d5-d7b95de37b5f', metadata={'source': './test.txt'}, page_content='than that with the double-sided tape while providing other benefits.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}, search_type=\"mmr\")\n",
    "\n",
    "# Fetch relevant documents\n",
    "query_user = retriever.invoke(\"what is the material weighs?\")\n",
    "query_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b03d5ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='76.50%', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-27T10:41:51.121697548Z', 'done': True, 'done_reason': 'stop', 'total_duration': 43107632741, 'load_duration': 48364827, 'prompt_eval_count': 295, 'prompt_eval_duration': 41844492861, 'eval_count': 5, 'eval_duration': 1214123684, 'model_name': 'llama3.1'}, id='run--8a38235a-c35e-4c65-bbfd-0a433dbaa03b-0', usage_metadata={'input_tokens': 295, 'output_tokens': 5, 'total_tokens': 300})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based on only the following context:\n",
    "                                          {context}\n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "#Fetch relevant documents\n",
    "docs = retriever.get_relevant_documents(\"what is the average intersession word accuracy?\")\n",
    "\n",
    "#Run\n",
    "chain.invoke({\"context\": docs, \"question\": \"what is the average intersession word accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7ed4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='76.50% and 68.18%.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-27T13:22:59.28521524Z', 'done': True, 'done_reason': 'stop', 'total_duration': 47126228057, 'load_duration': 48121590, 'prompt_eval_count': 309, 'prompt_eval_duration': 44248532861, 'eval_count': 11, 'eval_duration': 2828484865, 'model_name': 'llama3.1'}, id='run--8334bd70-1d50-4c47-a738-c8b8528482d4-0', usage_metadata={'input_tokens': 309, 'output_tokens': 11, 'total_tokens': 320})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based on only the following context:\n",
    "                                          {context}\n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    #fetch relevant docs\n",
    "    docs = retriever.get_relevant_documents(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "#run\n",
    "qa.invoke(input=\"what is the average intersession word accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "959adda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have information about your activities or the news you were reading. However, according to the provided context, an average intersession word accuracy of 76.50% was obtained using a bidirectional long short-term memory network for classification.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-27T17:17:25.821047598Z', 'done': True, 'done_reason': 'stop', 'total_duration': 68874507486, 'load_duration': 52761491, 'prompt_eval_count': 322, 'prompt_eval_duration': 54213159690, 'eval_count': 50, 'eval_duration': 14608016512, 'model_name': 'llama3.1'}, id='run--00e1cabf-2a90-420a-aff0-1fb668dc0165-0', usage_metadata={'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search query without additional info .\n",
    "                                                   end the queries with '**'.question:{x} Answer:\"\"\")\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\"').strip('**')\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "    new_query = rewriter.invoke(input)\n",
    "    #fetch relevant doc\n",
    "    docs = retriever.get_relevant_documents(new_query)\n",
    "    #format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "#run\n",
    "qa_rrr.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker.what is the average intersession word accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c047da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='76.50% for classification, and 68.18% in general.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-28T18:42:00.579047422Z', 'done': True, 'done_reason': 'stop', 'total_duration': 47412686188, 'load_duration': 46268909, 'prompt_eval_count': 291, 'prompt_eval_duration': 42806711645, 'eval_count': 17, 'eval_duration': 4559208152, 'model_name': 'llama3.1'}, id='run--ba8be1d2-fb97-4fc8-bca9-ddef70ad0e0e-0', usage_metadata={'input_tokens': 291, 'output_tokens': 17, 'total_tokens': 308})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based  similarity search. \n",
    "    Provide these alternative questions separated by newlines. \n",
    "    Original question: {question}\"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    #Flatten lis of lists, and dedupe them\n",
    "    deduped_docs = {doc.page_content: doc\n",
    "                    for sublist in document_lists for doc in sublist}\n",
    "    #return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based on this context:\n",
    "                                          {context}\n",
    "                                          Question: {question}\n",
    "                                          \"\"\")\n",
    "@chain \n",
    "def multi_query_qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({\"context\": docs , \"question\": input})\n",
    "    answer=llm.invoke(formatted)\n",
    "    return answer\n",
    "#run\n",
    "multi_query_qa.invoke(\"\"\"what is the average intersession word accuracy?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941cd4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='76.50% (for classification) and 68.18%', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-08-28T19:10:17.750903374Z', 'done': True, 'done_reason': 'stop', 'total_duration': 56086524028, 'load_duration': 56602135, 'prompt_eval_count': 355, 'prompt_eval_duration': 51970466952, 'eval_count': 15, 'eval_duration': 4058949872, 'model_name': 'llama3.1'}, id='run--46ce5aa5-12dc-45b9-824c-3ca194f2b3e4-0', usage_metadata={'input_tokens': 355, 'output_tokens': 15, 'total_tokens': 370})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "                                                      \\n Generate multiple search queries related to: {question} \n",
    "                                                     \\n Output (4 queries):\"\"\")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model='llama3.1')\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents and an optional parameter k used in the RRF formula\"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "multi_query_qa.invoke(\"what is the average intersession word accuracy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9f1a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyde\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "According to the provided context, the average intersession word accuracy is 72.34%. This value is not explicitly stated in the context, but it can be calculated by taking the average of the two values mentioned for each pair of documents with similar content: \n",
      "\n",
      "76.50% (from Document '7d367161-eda2-42b9-a487-e4634ef0f9a0') and 68.18% (also from Document '7d367161-eda2-42b9-a487-e4634ef0f9a0').\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to answer the question.\\n\n",
    "                                                Question: {question}\\n\n",
    "                                                Passage:\"\"\")\n",
    "\n",
    "generate_doc = (prompt_hyde | llm | StrOutputParser())\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "query = \"what is the average intersession word accuracy?\"\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9320f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
