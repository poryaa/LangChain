{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ceacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, SystemMessage\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "\n",
    "generate_prompt = SystemMessage(\"\"\"You are an essay assistant tasked with writingm excellent 3-paragraph essays.\"\"\"\n",
    "                                \"\"\"Generate the best possible essay for the users request. If the user provides critique, respond with a revised version of your previous attempts\"\"\")\n",
    "\n",
    "def generate(state:State) -> State:\n",
    "    answer = model.invoke([generate_prompt] + state[\"messages\"])\n",
    "    return {\"messages\":[answer]}\n",
    "\n",
    "reflection_prompt = SystemMessage(\"\"\"You are a teacher grading an essay submission. Generate critique and\n",
    "                                  recommendations for the users submission. Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\")\n",
    "\n",
    "def reflect(state:State) -> State:\n",
    "    cls_map ={AIMessage: HumanMessage , HumanMessage:AIMessage}\n",
    "    translated = [reflection_prompt, state[\"messages\"][0]] + \\\n",
    "        [cls_map[msg.__class__](content=msg.content) for msg in state[\"messages\"][1:]]\n",
    "    answer = model.invoke(translated)\n",
    "    return {\"messages\": [HumanMessage(content=answer.content)]}\n",
    "\n",
    "def should_continue(state:State) -> State:\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        return END\n",
    "    else:\n",
    "        return \"reflect\"\n",
    "    \n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_node(\"reflect\", reflect)\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_conditional_edges(\"generate\", should_continue,{\"reflect\": \"reflect\", END: END})\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Write an essay about the relevance of 'The Little Prince' today.\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "for output in graph.stream(initial_state):\n",
    "    message_type = \"generate\" if \"generate\" in output else \"reflect\"\n",
    "    print(\"\\nNew message:\", output[message_type]\n",
    "          [\"messages\"][-1].content[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484732e4",
   "metadata": {},
   "source": [
    "# Subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef00beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo : str\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str\n",
    "    bar: str\n",
    "def subgraph_node(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "subgraph_builder =StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd91c1a",
   "metadata": {},
   "source": [
    "# Supervisor Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"  # Fully open, no auth required\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "input_text = \"Hello, how are you today?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20fa92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
