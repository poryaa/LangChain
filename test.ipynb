{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9213be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I said that the capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-14T17:19:34.459274199Z', 'done': True, 'done_reason': 'stop', 'total_duration': 284490183, 'load_duration': 73147643, 'prompt_eval_count': 50, 'prompt_eval_duration': 35648202, 'eval_count': 11, 'eval_duration': 166846873, 'model_name': 'llama3.1:latest'}, id='lc_run--019c5d2a-391e-70f2-9dc2-030355039ae6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 50, 'output_tokens': 11, 'total_tokens': 61})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "promt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "chain = promt | chat_model\n",
    "\n",
    "chain.invoke(\n",
    "    {\"messages\": [(\"human\", \"What is the capital of France?\"),\n",
    "                  ('ai', \"The capital of France is Paris.\"),\n",
    "                  (\"human\", \"What did you say?\")]\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900dd556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chatbot': {'messages': [AIMessage(content='Hello Porya! Nice to meet you. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-14T18:04:15.622264047Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1796498871, 'load_duration': 1378463442, 'prompt_eval_count': 18, 'prompt_eval_duration': 22355648, 'eval_count': 26, 'eval_duration': 379145321, 'model_name': 'llama3.1:latest'}, id='lc_run--019c5d53-1c81-7b90-9697-2b5f50c18ba3-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 18, 'output_tokens': 26, 'total_tokens': 44})]}}\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_mdoel = ChatOllama(model=\"llama3.1:latest\")\n",
    "def chatbot(state: State):\n",
    "    answer = chat_mdoel.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "png_bytes = graph.get_graph().draw_mermaid_png()\n",
    "display(Image(png_bytes))\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inp = {\"messages\": [HumanMessage(content=\"hi! I am Porya.\")]}\n",
    "for chunk in graph.stream(inp):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a43fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (current response): {'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='9c7c359e-39ea-48a0-9d82-b647b599c811'), AIMessage(content=\"Hi Jack! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\", additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-14T18:08:02.955557342Z', 'done': True, 'done_reason': 'stop', 'total_duration': 561377682, 'load_duration': 73197889, 'prompt_eval_count': 17, 'prompt_eval_duration': 38728223, 'eval_count': 29, 'eval_duration': 431604721, 'model_name': 'llama3.1:latest'}, id='lc_run--019c5d56-9959-7f10-980e-e4cf85d74712-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 29, 'total_tokens': 46})]}\n",
      "Result 2 (current response): {'messages': [HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='24c88078-3083-49b8-bdb7-c8336c70542a'), AIMessage(content=\"I'm happy to chat with you, but I don't actually know your name. This conversation just started, and I don't have any prior information about you. Would you like to tell me what your name is?\", additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-14T18:08:03.689003447Z', 'done': True, 'done_reason': 'stop', 'total_duration': 731575046, 'load_duration': 64960715, 'prompt_eval_count': 15, 'prompt_eval_duration': 19131056, 'eval_count': 45, 'eval_duration': 621712301, 'model_name': 'llama3.1:latest'}, id='lc_run--019c5d56-9b8c-7403-bc1d-83f131de3968-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 15, 'output_tokens': 45, 'total_tokens': 60})]}\n"
     ]
    }
   ],
   "source": [
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run first interaction\n",
    "result_1 = graph.invoke({\"messages\": [HumanMessage(\"hi, my name is Jack!\")]}, config=thread1)\n",
    "print(\"Result 1 (current response):\", result_1)\n",
    "\n",
    "# Run follow-up interaction\n",
    "result_2 = graph.invoke({\"messages\": [HumanMessage(\"what is my name?\")]}, config=thread1)\n",
    "print(\"Result 2 (current response):\", result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4bcb637",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIMessage, HumanMessage, SystemMessage\n\u001b[32m      3\u001b[39m \u001b[38;5;28minput\u001b[39m = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(\u001b[33m'\u001b[39m\u001b[33mhi! I am Porya.\u001b[39m\u001b[33m'\u001b[39m)]}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mchatbot\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchatbot\u001b[39m(state: State) -> State:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     answer = \u001b[43mchat_mdoel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [answer]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:403\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    402\u001b[39m             \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m                 [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    404\u001b[39m                 stop=stop,\n\u001b[32m    405\u001b[39m                 callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m                 tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m                 metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    408\u001b[39m                 run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    409\u001b[39m                 run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    410\u001b[39m                 **kwargs,\n\u001b[32m    411\u001b[39m             ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LangChain/.langchain_venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:386\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001b[32m    382\u001b[39m msg = (\n\u001b[32m    383\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    385\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "During task with name 'chatbot' and id '66cfa8f3-96d1-6db5-37c4-3766fb02d3eb'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage('hi! I am Porya.')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ce475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader('test.txt')\n",
    "raw_text = loader.load()\n",
    "\n",
    "# Split the document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(raw_text)\n",
    "\n",
    "# Initialize the Ollama Embeddings model\n",
    "embeddings_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "# Connect to the PostgreSQL database with pgvector extension\n",
    "connection_string = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(chunks, embeddings_model, connection=connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c778b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In response to the report, the city council was split. Some members accepted the findings and pushed for a “convenience‑first” strategy that would restore some direct routes, extend evening and night services on key corridors, and introduce an integrated ticket that allowed unlimited transfers within 60 minutes at no extra cost. Others argued that the city’s budget could not support such measures without either significant tax increases or cuts to other services. A particularly contentious debate erupted around the idea of reinstating the direct tram line between Hohenfeld and the central station. Critics pointed out that the tram infrastructure had been partially dismantled and that rebuilding it would require a large upfront investment, while supporters claimed that the corridor’s high population density and strong latent demand justified a long‑term commitment.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = db.as_retriever()\n",
    "# results = retriever.invoke(\"In which year did Langenfurt face a mounting crisis in its public transportation system?\",k=1)\n",
    "# results[0].page_content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't know.\" additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-04T12:19:43.921133472Z', 'done': True, 'done_reason': 'stop', 'total_duration': 179140801, 'load_duration': 63796348, 'prompt_eval_count': 82, 'prompt_eval_duration': 38471296, 'eval_count': 6, 'eval_duration': 71998737, 'model_name': 'llama3.1:latest'} id='run--e808113d-81e2-4749-bea9-0b6b3ea4ad61-0' usage_metadata={'input_tokens': 82, 'output_tokens': 6, 'total_tokens': 88}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatOllama(model='llama3.1:latest',temperature=0)\n",
    "\n",
    "prompt =ChatPromptTemplate.from_template(\"\"\"Answer the question based on the context below./\n",
    "                                        If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "chain = prompt | chat_model\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "question=\"Who was the policy analyst in the Department of Urban Mobility?\"\n",
    "docs =  retriever.invoke(question,k=4)\n",
    "#run the chain\n",
    "response = chain.invoke({\"context\": docs[0].page_content, \"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4239334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DOC 0 ===\n",
      "\n",
      "gains but may also produce a sense of inequality if not accompanied by a broader, clearly communicated long‑term vision.\n",
      "\n",
      "=== DOC 1 ===\n",
      "\n",
      "gains but may also produce a sense of inequality if not accompanied by a broader, clearly communicated long‑term vision.\n",
      "\n",
      "=== DOC 2 ===\n",
      "\n",
      "Dr. Krauss’s report argued that the city had fallen into what she called the “efficiency trap”: a pattern in which short‑term cost‑cutting measures, justified by the language of optimization, led to long‑term losses in ridership and public trust. By focusing narrowly on metrics like cost per vehicle‑kilometer and average occupancy per vehicle, Langenfurt’s decision‑makers had overlooked less easily quantifiable factors such as perceived reliability, simplicity of routes, and the emotional comfort of not having to worry about missed connections late at night. She emphasized that once riders abandon public transport and adapt their lives around car use or alternative arrangements, it becomes far harder—and more expensive—to win them back.\n",
      "\n",
      "=== DOC 3 ===\n",
      "\n",
      "Dr. Krauss’s report argued that the city had fallen into what she called the “efficiency trap”: a pattern in which short‑term cost‑cutting measures, justified by the language of optimization, led to long‑term losses in ridership and public trust. By focusing narrowly on metrics like cost per vehicle‑kilometer and average occupancy per vehicle, Langenfurt’s decision‑makers had overlooked less easily quantifiable factors such as perceived reliability, simplicity of routes, and the emotional comfort of not having to worry about missed connections late at night. She emphasized that once riders abandon public transport and adapt their lives around car use or alternative arrangements, it becomes far harder—and more expensive—to win them back.\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print(f\"\\n=== DOC {i} ===\\n\")\n",
    "    print(d.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10794f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 (causal reasoning)\n",
    "# Q: According to the text, what deeper cause did Dr. Krauss identify behind the long‑term decline in public transport ridership in Langenfurt?\n",
    "\n",
    "# A: She concluded that the main cause was a series of policy decisions that prioritized short‑term cost‑cutting and “efficiency” over convenience, reliability, and simplicity for riders, which gradually pushed people away from the system.\n",
    "\n",
    "# Q2 (specific detail)\n",
    "# Q: What change in 2008 led to a 37 percent drop in daily ridership on the Hohenfeld corridor within a year?\n",
    "\n",
    "# A: The city replaced the direct tram line between Hohenfeld and the central train station with a bus route that required a transfer downtown.\n",
    "\n",
    "# Q3 (interpretation)\n",
    "# Q: What does the “efficiency trap” mean in the context of Langenfurt’s transit policy?\n",
    "\n",
    "# A: It refers to the pattern where measures intended to cut costs and optimize operations, like reducing direct routes or simplifying the network on paper, ended up undermining rider experience, causing ridership losses and making the system less sustainable in the long run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
