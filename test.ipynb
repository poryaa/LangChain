{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f84f6401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...blue! (at least, on a clear'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "llm_model = Ollama(model=\"llama3.1:latest\",\n",
    "                    temperature=0.7,\n",
    "                    num_predict=10)\n",
    "llm_model.invoke(\"the sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1a9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your AI: I'm functioning properly, thank you for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always ready to help answer your questions or chat with you. How about you? How's your day going so far?\", additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-01-29T10:50:29.540816311Z', 'done': True, 'done_reason': 'stop', 'total_duration': 998564892, 'load_duration': 70511542, 'prompt_eval_count': 30, 'prompt_eval_duration': 39861159, 'eval_count': 58, 'eval_duration': 853115530, 'model_name': 'llama3.1:latest'}, id='run--a16e03a2-196c-4b32-9f85-f2852924f8f5-0', usage_metadata={'input_tokens': 30, 'output_tokens': 58, 'total_tokens': 88})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "system_msg = {\"role\":\"system\", \"content\":\"always start an asnwer with Your AI:\"}\n",
    "usr_msg={\"role\":\"user\", \"content\":\"Hello, how are you?\"}\n",
    "chat_model.invoke([system_msg, usr_msg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e75fdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "template = ChatPromptTemplate.from_template(\"\"\"Answer the question based on the context. if not included, say \"I don't know.\" \"\"\"\n",
    "                                            \"Context:{context}\\n.\" \\\n",
    "                                            \"Question: {question}\\n\" \\\n",
    "                                            \"Answer:\")\n",
    "chain = template | chat_model\n",
    "\n",
    "context=\"x=4,y=10,z=15\"\n",
    "question=\"What is the value of w?\"\n",
    "chain.invoke({\"context\":context, \"question\":question}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95583c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x75d8044f0e50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list = [\"what is x?\", \"what is w?\"]\n",
    "\n",
    "inputs = [{\"context\": context, \"question\": q} for q in query_list]\n",
    "\n",
    "outputs = chain.stream(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1b614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
