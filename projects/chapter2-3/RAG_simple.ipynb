{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded62aad",
   "metadata": {},
   "source": [
    "# SIMPLE RAG\n",
    "here we want to get 5 differnet files generated by AI and then saved the embeddings in a pgvector vectorbase via a docker file and then try to have our chatbot based on them.\n",
    "volume_name: - pgvector_RAG_chp2data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebe218",
   "metadata": {},
   "source": [
    "\n",
    "## 1- Ingestion process\n",
    "![Ingestion process](ingest.png)\n",
    "\n",
    "### 1-1 Import 5 different docs \n",
    "* 1. Fairy story: Alice Enchanted Heart\n",
    "* 2. Blood Pressure and Heart Attack\n",
    "* 3. Medal of olympic games in 2012\n",
    "* 4. star movie facts and interesting points\n",
    "* 5. Tesla revenue 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use langchain different loaders to load documents from various sources\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader,UnstructuredMarkdownLoader\n",
    "\n",
    "doc_1 = PyPDFLoader(\"./1_Alice_story.pdf\").load()\n",
    "doc_2 = PyPDFLoader(\"./2_Medical_paper.pdf\").load()\n",
    "doc_3 = TextLoader(\"./3_olympics2012.txt\").load()\n",
    "doc_4 = UnstructuredMarkdownLoader(\"./4_star_war.md\").load()\n",
    "doc_5 = PyPDFLoader(\"./5_TeslaRevenue2023.pdf\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28765e06",
   "metadata": {},
   "source": [
    "### 1-2 splitt docs in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use langchain text splitter to split the documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#Combine all documents into a single list of texts(with concatenation)\n",
    "docs = doc_1 + doc_2 + doc_3 + doc_4 + doc_5\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "splitted_texts = text_splitter.split_documents(docs)\n",
    "\n",
    "# outputs\n",
    "pprint(f\"The number of original documents: {len(docs)} and after splitting there are {len(splitted_texts)} of chunks.\")\n",
    "pprint(\"Here is an example of a splitted text chunk metadata:\")\n",
    "pprint(splitted_texts[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad288b",
   "metadata": {},
   "source": [
    "### 1-3 Embedding the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ollama as embedding model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# the pgvector will do the embeddings for us! so next section will do it\n",
    "# embeddings = embedding_model.embed_documents([chunk.page_content for chunk in splitted_texts])\n",
    "\n",
    "# print(f\"The dimension of the embeddings matrix are(rows*columns): {len(embeddings)}*{len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c162e",
   "metadata": {},
   "source": [
    "### 1-4 Save in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf402613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now first we need a docker to build the vector database.\n",
    "# we use pgvector dokcer (create a docker-compose.yml file and write these:)\n",
    "\n",
    "# services:\n",
    "#   pgvector:\n",
    "#     image: pgvector/pgvector:pg16\n",
    "#     container_name: pgvector-container\n",
    "#     environment:\n",
    "#       POSTGRES_USER: langchain\n",
    "#       POSTGRES_PASSWORD: langchain\n",
    "#       POSTGRES_DB: langchain\n",
    "#       POSTGRES_HOST_AUTH_METHOD: md5\n",
    "#     ports:\n",
    "#       - \"6024:5432\"\n",
    "#     volumes:\n",
    "#       - pgvector_RAG_chp2data:/var/lib/postgresql/data\n",
    "\n",
    "# volumes:\n",
    "#   pgvector_RAG_chp2data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =====================Error handling====================================\n",
    "# #clean the documents from null characters\n",
    "# def clean_doc(doc):\n",
    "#     doc.page_content = doc.page_content.replace(\"\\x00\", \"\")\n",
    "#     return doc\n",
    "# cleaned_docs = [clean_doc(d) for d in splitted_texts]\n",
    "# # ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "# db = PGVector.from_documents(cleaned_docs, embedding_model, connection=connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"What is the name of the village where Alice lives?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call vectore database again to re-use the existing store, no reinsertion\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55edab",
   "metadata": {},
   "source": [
    "## 2- RAG system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f65db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke the db vectorebase to do similarity search again\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e577f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The United States topped the medal table at London 2012, winning 46 gold medals, 29 silver medals, and 29 bronze medals, for a total of 104 medals.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-10T16:11:00.773938825Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1260135223, 'load_duration': 61405010, 'prompt_eval_count': 2032, 'prompt_eval_duration': 593477488, 'eval_count': 39, 'eval_duration': 582585056, 'model_name': 'llama3.1:latest'}, id='lc_run--019c4852-0039-7122-b227-f0bcbd6fa448-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2032, 'output_tokens': 39, 'total_tokens': 2071})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "llm_model =ChatOllama(model='llama3.1:latest')\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question short based on the following retrieved documents:\n",
    "                                           and if you don't know the answer say you I don't know.\n",
    "                                           Context: {context}\n",
    "                                           Question: {question}\n",
    "                                           Answer: \"\"\")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "@chain\n",
    "def q_a(input):\n",
    "    #fetch releavante doc for the question\n",
    "    docs = retriever.invoke(input)\n",
    "    #format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm_model.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "q_a.invoke(input=\"who wins the olympics 2012? \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
