{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae27cb46",
   "metadata": {},
   "source": [
    "# RAG with Memory and rewriter llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fedb9",
   "metadata": {},
   "source": [
    "## Main system process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke the db vectorebase to do similarity search again\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d10bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langgraph technique + adding memory + \n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add the chat node\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant for questions about our colleagues.\n",
    "Answer **only** using the information in CONTEXT.\n",
    "If the answer is not in CONTEXT, say exactly: \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 15})\n",
    "\n",
    "def chat_bot(state: State):\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    question = last_msg.content if isinstance(last_msg, HumanMessage) else str(last_msg)\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    # formatted_prompt is a string; for ChatOllama you can pass it directly\n",
    "    answer_msg = llm_model.invoke(formatted_prompt)\n",
    "\n",
    "    return {\"messages\": [answer_msg]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chat_bot)\n",
    "\n",
    "\n",
    "# def chat_bot(state: State):\n",
    "#     answer = llm_model.invoke(state[\"messages\"])\n",
    "#     return {\"messages\": [answer]}\n",
    "# builder.add_node(\"chatbot\", chat_bot)\n",
    "\n",
    "# Add the edges\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# #plot the graph\n",
    "# from IPython.display import Image, display\n",
    "# png_bytes = graph.get_graph().draw_mermaid_png()\n",
    "# display(Image(png_bytes))\n",
    "# Run the graph\n",
    "# create the thread and invoke the graph with the thread\n",
    "thread_1 = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "# input = {\"messages\": [\"Hi. my name is Porya.\"]}\n",
    "# result_1 = graph.invoke(input, thread_1)\n",
    "\n",
    "# input = {\"messages\": [\"what was my name?\"]}\n",
    "# result_2 = graph.invoke(input, thread_1)\n",
    "# graph.get_state(thread_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "833f89cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Who has this number: +49 151 2904 1718 ?', additional_kwargs={}, response_metadata={}, id='0a77fcb8-0512-4d1f-af91-4d1fed4448fc'),\n",
       "  AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-16T12:39:55.848617574Z', 'done': True, 'done_reason': 'stop', 'total_duration': 926736411, 'load_duration': 60350568, 'prompt_eval_count': 2546, 'prompt_eval_duration': 771715423, 'eval_count': 6, 'eval_duration': 87702416, 'model_name': 'llama3.1:latest'}, id='lc_run--019c6676-e929-7f70-aa3a-3fcca767466e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2546, 'output_tokens': 6, 'total_tokens': 2552})]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"messages\": [\"Who has this number: +49 151 2904 1718 ?\"]}\n",
    "result = graph.invoke(input, thread_1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14151b47",
   "metadata": {},
   "source": [
    "## A single LLM just to test the info it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a76d3e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pouriya Amini Digehsara studied at Foolad Novin Academy and also played for Foolad, before joining Persepolis in Iran.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-16T12:36:38.326059819Z', 'done': True, 'done_reason': 'stop', 'total_duration': 576523509, 'load_duration': 63803227, 'prompt_eval_count': 46, 'prompt_eval_duration': 16639762, 'eval_count': 33, 'eval_duration': 475479119, 'model_name': 'llama3.1:latest'}, id='lc_run--019c6673-e6f4-72e0-880d-e52561eace92-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 46, 'output_tokens': 33, 'total_tokens': 79})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMPORARY: Just a single llm to check if it knows the answer\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.Your name is Rosenxt_bot.answer the question.\"),\n",
    "    ('human', \"question: {Question}\")\n",
    "])\n",
    "\n",
    "chain = template | llm_model\n",
    "chain.invoke({\"Question\": \"where did Pouriya Amini Digehsara study?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ff2f1",
   "metadata": {},
   "source": [
    "## Vector data base and embedding prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c89fad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of original documents: 1 and after splitting there are 39 of '\n",
      " 'chunks.')\n",
      "'Here is an example of a splitted text chunk metadata:'\n",
      "{'author': 'LinkedIn',\n",
      " 'creationdate': '2026-02-16T09:50:41+00:00',\n",
      " 'creator': 'PyPDF',\n",
      " 'page': 1,\n",
      " 'page_label': '2',\n",
      " 'producer': 'Apache FOP Version 2.3',\n",
      " 'source': 'data/Dirk.pdf',\n",
      " 'subject': 'Resume generated from profile',\n",
      " 'title': 'Resume',\n",
      " 'total_pages': 2}\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the pdfs of the profile of our collegous for RAG\n",
    "\n",
    "#1-load pdfs\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "DATA_PATH = Path(\"./data/\")  # adjust to teh data folder\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(str(DATA_PATH / file))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "#2- split docs to chunks\n",
    "# use langchain text splitter to split the documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "splitted_texts = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# outputs\n",
    "pprint(f\"The number of original documents: {len(docs)} and after splitting there are {len(splitted_texts)} of chunks.\")\n",
    "pprint(\"Here is an example of a splitted text chunk metadata:\")\n",
    "pprint(splitted_texts[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- embdeeing chunks and save in vector database\n",
    "\n",
    "# firtst let create the database via docker (docker.yml file):\n",
    "# services:\n",
    "#   pgvector:\n",
    "#     image: pgvector/pgvector:pg16\n",
    "#     container_name: pgvector-container-chp4\n",
    "#     environment:\n",
    "#       POSTGRES_USER: langchain\n",
    "#       POSTGRES_PASSWORD: langchain\n",
    "#       POSTGRES_DB: langchain\n",
    "#       POSTGRES_HOST_AUTH_METHOD: md5\n",
    "#     ports:\n",
    "#       - \"6024:5432\"\n",
    "#     volumes:\n",
    "#       - pgvector_RAG_chp4:/var/lib/postgresql/data\n",
    "\n",
    "# volumes:\n",
    "#   pgvector_RAG_chp4:\n",
    "\n",
    "# then we can use pgvector to save our chunks(first time:)\n",
    "# from langchain_postgres.vectorstores import PGVector\n",
    "# # using ollama as embedding model\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "# db = PGVector.from_documents(splitted_texts, embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call vectore database again to re-use the existing store, no reinsertion\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
