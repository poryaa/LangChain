{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae27cb46",
   "metadata": {},
   "source": [
    "# RAG with Memory and rewriter llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fedb9",
   "metadata": {},
   "source": [
    "## Main system process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127829/844835462.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "#invoke the db vectorebase to do similarity search again\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10bfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langgraph technique + adding memory + \n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add the chat node\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant for questions about our colleagues.\n",
    "First, use the information in CONTEXT as your primary source.\n",
    "Try to asnwer in a short way.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 15})\n",
    "\n",
    "def chat_bot(state: State):\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    question = last_msg.content if isinstance(last_msg, HumanMessage) else str(last_msg)\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    # formatted_prompt is a string; for ChatOllama you can pass it directly\n",
    "    answer_msg = llm_model.invoke(formatted_prompt)\n",
    "\n",
    "    return {\"messages\": [answer_msg]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chat_bot)\n",
    "\n",
    "\n",
    "# def chat_bot(state: State):\n",
    "#     answer = llm_model.invoke(state[\"messages\"])\n",
    "#     return {\"messages\": [answer]}\n",
    "# builder.add_node(\"chatbot\", chat_bot)\n",
    "\n",
    "# Add the edges\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# #plot the graph\n",
    "# from IPython.display import Image, display\n",
    "# png_bytes = graph.get_graph().draw_mermaid_png()\n",
    "# display(Image(png_bytes))\n",
    "# Run the graph\n",
    "# create the thread and invoke the graph with the thread\n",
    "thread_1 = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "# input = {\"messages\": [\"Hi. my name is Porya.\"]}\n",
    "# result_1 = graph.invoke(input, thread_1)\n",
    "\n",
    "# input = {\"messages\": [\"what was my name?\"]}\n",
    "# result_2 = graph.invoke(input, thread_1)\n",
    "# graph.get_state(thread_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "833f89cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Who is Pouriya Amini?', additional_kwargs={}, response_metadata={}, id='e8bc5b45-950d-44b5-863b-cf6451b2c5c2'),\n",
       "  AIMessage(content='Pouriya Amini Digehsara, a Data scientist | AI/ML engineer based in Lingen (Ems), Germany.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-26T13:32:09.408857049Z', 'done': True, 'done_reason': 'stop', 'total_duration': 860173014, 'load_duration': 69149490, 'prompt_eval_count': 1904, 'prompt_eval_duration': 356239740, 'eval_count': 29, 'eval_duration': 413108494, 'model_name': 'llama3.1:latest'}, id='lc_run--019c9a26-51e3-7f60-a0ab-985fce339eec-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1904, 'output_tokens': 29, 'total_tokens': 1933}),\n",
       "  HumanMessage(content='what is Mobile of  Pouriya Amini?', additional_kwargs={}, response_metadata={}, id='47c7cd09-4f5a-470a-9004-1b60cca4dccf'),\n",
       "  AIMessage(content='(+49)17661053413 (Mobile)', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-26T13:32:22.47069283Z', 'done': True, 'done_reason': 'stop', 'total_duration': 708049761, 'load_duration': 69688897, 'prompt_eval_count': 1829, 'prompt_eval_duration': 480673305, 'eval_count': 11, 'eval_duration': 148581459, 'model_name': 'llama3.1:latest'}, id='lc_run--019c9a26-8581-72a2-b804-e0452e9f5a08-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1829, 'output_tokens': 11, 'total_tokens': 1840}),\n",
       "  HumanMessage(content='what is Mobile of Basel?', additional_kwargs={}, response_metadata={}, id='ac97571d-7826-42af-88a3-184649cb4c8d'),\n",
       "  AIMessage(content='According to the CONTEXT, the mobile number of Basel Alhaji is +49 151 2904 1718.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-26T13:34:27.164280248Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1103193198, 'load_duration': 68867066, 'prompt_eval_count': 2113, 'prompt_eval_duration': 620533861, 'eval_count': 26, 'eval_duration': 396741629, 'model_name': 'llama3.1:latest'}, id='lc_run--019c9a28-6b0c-77b0-a9cf-946222b74a3b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2113, 'output_tokens': 26, 'total_tokens': 2139}),\n",
       "  HumanMessage(content='what is the job title of Sebastian?', additional_kwargs={}, response_metadata={}, id='936a6faa-0a2b-46de-8aa7-4401cd0c10e4'),\n",
       "  AIMessage(content='CTO (Chief Technology Officer) at Rosenxt Group.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-26T13:35:07.391774774Z', 'done': True, 'done_reason': 'stop', 'total_duration': 848652215, 'load_duration': 64160531, 'prompt_eval_count': 1903, 'prompt_eval_duration': 545864590, 'eval_count': 13, 'eval_duration': 228017342, 'model_name': 'llama3.1:latest'}, id='lc_run--019c9a29-092e-7f82-9d7e-789eb8dc9e7d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1903, 'output_tokens': 13, 'total_tokens': 1916})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the job title of Sebastian?\"]}\n",
    "result = graph.invoke(input, thread_1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14151b47",
   "metadata": {},
   "source": [
    "## A single LLM just to test the info it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a76d3e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have enough information to determine the job title of Sebastian. Could you provide more context or details about which Sebastian you are referring to? This will help me give a more accurate answer.\", additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2026-02-26T13:35:25.941959146Z', 'done': True, 'done_reason': 'stop', 'total_duration': 710867316, 'load_duration': 68173324, 'prompt_eval_count': 41, 'prompt_eval_duration': 35648574, 'eval_count': 40, 'eval_duration': 581952938, 'model_name': 'llama3.1:latest'}, id='lc_run--019c9a29-522e-70f3-b6f2-1319c6eb8c3a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 40, 'total_tokens': 81})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMPORARY: Just a single llm to check if it knows the answer\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.Your name is Rosenxt_bot.answer the question.\"),\n",
    "    ('human', \"question: {Question}\")\n",
    "])\n",
    "\n",
    "chain = template | llm_model\n",
    "chain.invoke({\"Question\": \"what is the job title of Sebastian?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ff2f1",
   "metadata": {},
   "source": [
    "## Vector data base and embedding prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c89fad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of original documents: 1 and after splitting there are 68 of '\n",
      " 'chunks.')\n",
      "'Here is an example of a splitted text chunk metadata:'\n",
      "{'author': 'LinkedIn',\n",
      " 'creationdate': '2026-02-16T09:50:52+00:00',\n",
      " 'creator': 'PyPDF',\n",
      " 'page': 0,\n",
      " 'page_label': '1',\n",
      " 'producer': 'Apache FOP Version 2.3',\n",
      " 'source': 'data/Andrey.pdf',\n",
      " 'subject': 'Resume generated from profile',\n",
      " 'title': 'Resume',\n",
      " 'total_pages': 2}\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the pdfs of the profile of our collegous for RAG\n",
    "\n",
    "#1-load pdfs\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "DATA_PATH = Path(\"./data/\")  # adjust to teh data folder\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(str(DATA_PATH / file))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "#2- split docs to chunks\n",
    "# use langchain text splitter to split the documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "splitted_texts = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# outputs\n",
    "pprint(f\"The number of original documents: {len(docs)} and after splitting there are {len(splitted_texts)} of chunks.\")\n",
    "pprint(\"Here is an example of a splitted text chunk metadata:\")\n",
    "pprint(splitted_texts[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- embdeeing chunks and save in vector database\n",
    "\n",
    "# firtst lets create the database via docker (docker.yml file):\n",
    "# services:\n",
    "#   pgvector:\n",
    "#     image: pgvector/pgvector:pg16\n",
    "#     container_name: pgvector-container-chp4\n",
    "#     environment:\n",
    "#       POSTGRES_USER: langchain\n",
    "#       POSTGRES_PASSWORD: langchain\n",
    "#       POSTGRES_DB: langchain\n",
    "#       POSTGRES_HOST_AUTH_METHOD: md5\n",
    "#     ports:\n",
    "#       - \"6024:5432\"\n",
    "#     volumes:\n",
    "#       - pgvector_RAG_chp4:/var/lib/postgresql/data\n",
    "\n",
    "# volumes:\n",
    "#   pgvector_RAG_chp4:\n",
    "\n",
    "# =========================Naive embeddings================================\n",
    "# NOW we can use pgvector to save our chunks(first time:)\n",
    "# #Option1: using llama3.1 embedding model\n",
    "# from langchain_postgres.vectorstores import PGVector\n",
    "# # using ollama as embedding model\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "\n",
    "# connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "# db = PGVector.from_documents(splitted_texts, embedding_model, connection=connection)\n",
    "\n",
    "# # ==========================Best embeddings=========================\n",
    "# #Option2: using sentence-transformers model from Huggingface\n",
    "# from langchain_postgres.vectorstores import PGVector\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "# db = PGVector.from_documents(splitted_texts, embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call vectore database again to re-use the existing store, no reinsertion\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "# embedding_model = OllamaEmbeddings(model='llama3.1:latest')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Later, second time (re-use existing store, no reinsertion)\n",
    "db = PGVector.from_existing_index(embedding=embedding_model, connection=connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain_venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
