{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b8e4b0",
   "metadata": {},
   "source": [
    "# Review and rerun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43acd53",
   "metadata": {},
   "source": [
    "## Chapter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59874a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama without langchain\n",
    "import ollama\n",
    "result = ollama.generate(model=\"gemma3:1b\", prompt=\"why is the sly blue?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama inside the langchain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "llm.invoke(\"the sky is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama as chatbot\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "message = [{\"role\": \"system\", \"content\":\"You are a helpful translator. Translate the user sentence to german.\"},\n",
    "           {\"role\": \"user\", \"content\":\"I love learing french.\"}]\n",
    "\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f63aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "                                             (\"human\", \"{input}\")])\n",
    "\n",
    "chain = template | llm\n",
    "chain.invoke({\"input_language\":\"English\",\n",
    "              \"output_language\": \"German\",\n",
    "              \"input\": \"I love programming.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6951cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p8)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_template(\"\"\"You are a translator. translate the below Context:\".\n",
    "Context: {context}\n",
    "Language: {language}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p11)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    justification: str\n",
    "    '''justification for the answer'''\n",
    "    \n",
    "llm = ChatOllama(model='llama3.1')\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    '''An answer to the user question along with number of words for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    num_of_words: int\n",
    "    '''number of words for the answer'''\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\").with_structured_output(Answer)\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming and teaching.\",\n",
    "        \"language\": \"german\"})\n",
    "\n",
    "chain = template | model\n",
    "response =chain.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using runnable interface(invoke(), bathc(), stream())\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "#1 invoke()\n",
    "#llm.invoke(\"hello there!\")\n",
    "\n",
    "#2 batch()\n",
    "#llm.batch([\"hi there!\", \"what is your name?\"])\n",
    "\n",
    "#3 stream()\n",
    "# for i in llm.stream('Bye!'):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imperative Composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f7380",
   "metadata": {},
   "source": [
    "## Chapter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path=\"./test.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now splitting it to a number of chunks\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "splitted_docs = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd66f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length: \", len(splitted_docs), end='\\n\\n'),\n",
    "print(\"Page content: \", splitted_docs[100].page_content, end='\\n\\n')\n",
    "print(\"All metadata: \", splitted_docs[0].metadata, end='\\n\\n')\n",
    "print(\"An example of metadata: \", splitted_docs[0].metadata[\"page_label\"], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc60357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking when we have only raw data not a doc\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "text = (\"horse is for king\")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "horse = embedding_model.embed_documents(text)\n",
    "# len(vector[0])\n",
    "# vector[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use document loade, splitting text and embedding all combined\n",
    "#1- load the file\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#2- splitt the documnet\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#3- embedding the documnet\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "embeddings = embedding_model.embed_documents([chunk.page_content for chunk in splitted_docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splitted_docs), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for work labtop\n",
    "# docker run --name pgvector-container \\\n",
    "# -e POSTGRES_USER=langchain \\\n",
    "# -e POSTGRES_PASSWORD=langchain \\\n",
    "# -e POSTGRES_DB=langchain \\\n",
    "# -e POSTGRES_HOST_AUTH_METHOD=md5 \\\n",
    "# -p 6024:5432 \\\n",
    "# -v pgvector-data:/var/lib/postgresql/data \\\n",
    "# -d pgvector/pgvector:pg16\n",
    "\n",
    "# connection_string:\"d539c9988113e8335cb996537db3cbcaf12438bece7430b9e11da4a311b37bc0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGVector\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#split the documnet\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter()             #chunk_size=1000, chunk_overlap=100\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#embedding the documnet\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model=\"tinyllama:latest\")\n",
    "\n",
    "db = PGVector.from_documents(splitted_docs, embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e43fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the similarity (by using model embedding only)\n",
    "db.similarity_search(\"Ahrar Institute of Technology and Higher\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to add new data to the db\n",
    "import uuid\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "\n",
    "db.add_documents([\n",
    "                    Document(page_content=\"there are cats in the pond\",\n",
    "                            metadata={\"location\":\"pond\" ,\"topic\":\"animals\"}),\n",
    "                    Document(page_content=\"Ducks are also found in the pond\",\n",
    "                    metadata={\"location\":\"pond\" ,\"topic\":\"animals\"})],\n",
    "    ids=ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52708b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"are cats in the pond\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed736c65",
   "metadata": {},
   "source": [
    "## Chapter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p62 retrieve data\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#fetch relevant documents\n",
    "docs = retriever.invoke(\"\"\"What are the main components and specifications of the experimental fault simulator setup\n",
    "                         at Ahrar Institute of Technology and Higher Education (AITHE),\n",
    "                         and how were vibration signals collected and measured for different fault conditions?\"\"\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p64 retrieve data from db and run llm\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer the question only based on the following context: {context}, Question:{question}.\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "chain = prompt | llm\n",
    "#fetch relevant documents\n",
    "question= \"\"\" which institute were the experimental recordings in this setup obtained?\"\"\"\n",
    "docs = retriever.invoke(question, k=5)\n",
    "\n",
    "#run\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p65 encapsulate the retrieval logic\n",
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question only based on th following context:\n",
    "                                          context: {context},\n",
    "                                          question: {question}.\"\"\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    #fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    #format prompt\n",
    "    formatted_prompt = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "    return answer\n",
    "\n",
    "#run\n",
    "qa.invoke(\"\"\"What role does the frequency of intermittent impulses in a vibration signal play\n",
    "           in detecting bearing faults and identifying the location of defects within bearing components?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p74: RAG fusion for query transformation\n",
    "#part1\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant that generates multiple search queries\n",
    "                                                      based on a single input query. \\n\n",
    "                                                     generate multiple search queries related to: {question} \\n\n",
    "                                                     Output (4 queries): \"\"\")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen.invoke(\"which institute were the experimental recordings in this setup obtained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents and an optional parameter k used in the RRF formula\"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d32509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "rag_fusion.invoke(\"In which institute were the dataset recorded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p78: Hypothetical Document embedding\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to answer the question.\\n\n",
    "                                                Question: {question}\\n\n",
    "                                                Passage:\"\"\")\n",
    "\n",
    "generate_doc = (prompt_hyde | llm | StrOutputParser())\n",
    "\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "query = \"where the experimental set of the faults simulator in the rotating machines were recorded?\"\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bd117",
   "metadata": {},
   "source": [
    "# Chapter04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p96: store and use all previous messages for chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability\"),\n",
    "                                           (\"placeholder\",\"{messages}\")])\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"messages\":[\"human\",\"Translate this from English to German: I love programming.\",\n",
    "                          \"ai\",\"Ich liebe programmieren.\",\n",
    "                          \"human\",\"what did you say?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982de3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p101: Simple LangGraph for chatbot\n",
    "\n",
    "#part1: the langgraph itself\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START,END,StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aad33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2: the chatbot node\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2362b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3: add edges\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part4: run the graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "input = {\"messages\": [HumanMessage(\"hi!\")]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p105: adding memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "graph = builder.compile(checkpointer = MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d030996",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = {\"configurable\": {\"thread_id\":\"1\"}}\n",
    "result_1 = graph.invoke({\"messages\": [HumanMessage(\"hi my name is XXX.\")]},\n",
    "                        thread1)\n",
    "result_2 = graph.invoke({\"messages\": [HumanMessage(\"what is my name?\")]},\n",
    "                        thread1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f787e",
   "metadata": {},
   "source": [
    "# Chapter05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ed48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P118: Arcitecture#1: LLM Call\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph=builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "input={\"messages\": [HumanMessage('hi!')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p121: Arcitecture#2: Chain\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model_1 = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)#low temp to generate SQL query\n",
    "model_2 = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)#high temp to generate natural language outputs\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]# to track the state and conversations\n",
    "    user_query: str #input\n",
    "    sql_query: str #output\n",
    "    sql_explanation: str #output\n",
    "\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    sql_query: str\n",
    "    sql_explanation: str\n",
    "############generation part################\n",
    "generate_prompt = SystemMessage('''You are a helpful data analyst who generates SQL queries for users based on their questions.''' )\n",
    "def generate_sql(state: State):\n",
    "    user_msg = HumanMessage(state[\"user_query\"])\n",
    "    messages = [generate_prompt, *state[\"messages\"], user_msg]\n",
    "    res = model_1.invoke(messages)\n",
    "    return{\n",
    "        \"sql_query\": res.content,\n",
    "        \"messages\": [user_msg, res]# update converstion history\n",
    "    }\n",
    "############explanantion part################\n",
    "explain_prompt = SystemMessage('''You are a helpful data analyst who explains SQL queries to users''')\n",
    "def explain_sql(state: State):\n",
    "    messages = [explain_prompt, *state[\"messages\"]]\n",
    "    res = model_2.invoke(messages)\n",
    "    return{\n",
    "        \"sql_explanation\": res.content,\n",
    "        \"messages\": [*state[\"messages\"], res] # update converstion history\n",
    "    }\n",
    "############build LangGraph################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"generate_sql\", generate_sql)\n",
    "builder.add_node(\"explain_sql\", explain_sql)\n",
    "builder.add_edge(START,\"generate_sql\")\n",
    "builder.add_edge(\"generate_sql\", \"explain_sql\")\n",
    "builder.add_edge(\"explain_sql\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "graph.invoke({\"user_query\": \"What is the total sales for each product?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P126: Arcitecture#3: Router\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from typing import Annotated, TypedDict,Literal\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "model_low_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)\n",
    "model_high_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)\n",
    "\n",
    "############ State prepration ################\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_query: str\n",
    "    domain: Literal[\"records\", \"insurance\"]\n",
    "    documents: list[Document]\n",
    "    answer: str\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    documents: list[Document]\n",
    "\n",
    "############ Vectore store prepration ################\n",
    "medical_records_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "medical_records_retriever = medical_records_store.as_retriever()\n",
    "\n",
    "insurance_faqs_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "insurance_faqs_retriever = insurance_faqs_store.as_retriever()\n",
    "\n",
    "############ Router part ################\n",
    "router_prompt = SystemMessage(\n",
    "    \"\"\"You need to decide which domain to route the user query to. You have two domains to choose from:\n",
    "- records: contains medical records of the patient, such as diagnosis, treatment, and prescriptions.\n",
    "- insurance: contains frequently asked questions about insurance policies, claims, and coverage.\n",
    "\n",
    "Output only the domain name.\"\"\")\n",
    "def router_node(state: State) -> State:\n",
    "    user_message = HumanMessage(state[\"user_query\"])\n",
    "    messages = [router_prompt, *state[\"messages\"], user_message]\n",
    "    res = model_low_temp.invoke(messages)\n",
    "    return {\n",
    "        \"domain\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": [user_message, res],\n",
    "    }\n",
    "############ pick part ################\n",
    "def pick_retriever(\n",
    "    state: State,\n",
    ") -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        return \"retrieve_medical_records\"\n",
    "    else:\n",
    "        return \"retrieve_insurance_faqs\"\n",
    "    \n",
    "def retrieve_medical_records(state: State) -> State:\n",
    "    documents = medical_records_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_insurance_faqs(state: State) -> State:\n",
    "    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "############ retrieve part ################    \n",
    "medical_records_prompt = SystemMessage(\n",
    "    \"You are a helpful medical chatbot, who answers questions based on the patient's medical records, such as diagnosis, treatment, and prescriptions.\"\n",
    ")\n",
    "\n",
    "insurance_faqs_prompt = SystemMessage(\n",
    "    \"You are a helpful medical insurance chatbot, who answers frequently asked questions about insurance policies, claims, and coverage.\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: State) -> State:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        prompt = medical_records_prompt\n",
    "    else:\n",
    "        prompt = insurance_faqs_prompt\n",
    "    messages = [\n",
    "        prompt,\n",
    "        *state[\"messages\"],\n",
    "        HumanMessage(f\"Documents: {state['documents']}\"),\n",
    "    ]\n",
    "    res = model_high_temp.invoke(messages)\n",
    "    return {\n",
    "        \"answer\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": res,\n",
    "    }\n",
    "############ langgraph part ################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"router\", router_node)\n",
    "builder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\n",
    "builder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\"router\", pick_retriever)\n",
    "builder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\n",
    "builder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2d26c",
   "metadata": {},
   "source": [
    "# Chapter06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p139: Basic Agent architecture using a chat model and LangGraph\n",
    "\n",
    "#pip install duckduckgo-search\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a515cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chunk[\"chat_model\"][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56050025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P143: Always calling a tool first\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "def first_model(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_tool_call = ToolCall(name= \"DuckDuckGoSearchRun\", args={\"query\": query}, id = uuid4().hex)\n",
    "    return {\"messages\": AIMessage(content=\"\", tool_calls=[search_tool_call])}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"first_model\", first_model)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"first_model\")\n",
    "builder.add_edge(\"first_model\",\"tools\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a36057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P148: Dealing with many tools\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "llm_model =ChatOllama(model=\"llama3.1:latest\", temperature=0.1) \n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents([Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
    "                                                     embedding=embeddings_model).as_retriever()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state[\"selected_tools\"]]\n",
    "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "def selected_tools(state:State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return{'selected_tools': [doc.metadata[\"name\"] for doc  in tool_docs]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"select_tools\", selected_tools)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "builder.add_edge(\"select_tools\",\"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fe8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12777a22",
   "metadata": {},
   "source": [
    "# Chapter07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf23f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P156: Reflection for agents\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseModel], add_messages]\n",
    "\n",
    "generate_prompt = SystemMessage(\"\"\"You are an essay assistant tasked with writing excellent 3-paragraph essays.\"\"\"\n",
    "                                \"\"\"Generate the best essay possible for user's request.\"\"\"\n",
    "                                \"\"\"if the user provide critique, respond with a revised version of your previous attempts.\"\"\")\n",
    "def generate(state:State)->State:\n",
    "    answer=llm_model.invoke([generate_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "reflection_prompt = SystemMessage(\"\"\"You are a teacher grading an essay submission. generate critique and recommendations for the user's submission.\"\"\"\n",
    "                                  \"\"\"provide detailed recommendations, including requests for length, depth, style, etc.\"\"\")\n",
    "def reflect(state:State)->State:\n",
    "    cls_map = {AIMessage: HumanMessage, HumanMessage: AIMessage}\n",
    "    translated = [reflection_prompt, state[\"messages\"][0]] + [cls_map[msg.__class__](content=msg.content) for msg in state[\"messages\"][1:]]\n",
    "    answer = llm_model.invoke(translated)\n",
    "    return {\"messages\": [HumanMessage(content=answer.content)]}\n",
    "\n",
    "def should_continue(state:State):\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        return END\n",
    "    else:\n",
    "        return \"reflect\"\n",
    "    \n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_node(\"reflect\", reflect)\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_conditional_edges(\"generate\", should_continue, {\"reflect\":\"reflect\", END: END})\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "graph = builder.compile() \n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Write an essay about the relevance of 'The Little Prince' today.\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "for output in graph.stream(initial_state):\n",
    "    message_type = \"generate\" if \"generate\" in output else \"reflect\"\n",
    "    print(\"\\nNew message:\", output[message_type]\n",
    "          [\"messages\"][-1].content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61239747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P162: method1:calling a Subgraph directly\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph \n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node(state:SubgraphState)->State:\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "# subgraph_builder = StateGraph(SubgraphState)\n",
    "# subgraph_builder.add_node(subgraph_node)\n",
    "# #...\n",
    "# subgragh = subgraph_builder.compile()\n",
    "\n",
    "# #Define parent graph\n",
    "# builder = StateGraph(State)\n",
    "# builder.add_node(\"subgraph\", subgragh)\n",
    "# # ...\n",
    "# graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P164: method2:calling a Subgraph with a function\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "class SubgraphState(TypedDict):\n",
    "    baz: str\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node(state:SubgraphState):\n",
    "    return {\"bar\": state[\"bar\"] + \"baz\"}\n",
    "\n",
    "# subgraph_builder = StateGraph(SubgraphState)\n",
    "# subgraph_builder.add_node(subgraph_node)\n",
    "# #...\n",
    "# subgragh = subgraph_builder.compile()\n",
    "\n",
    "# def node(state: State):\n",
    "#     response = subgraph.invoke({\"bar\": state[\"foo\"]})\n",
    "#     return {\"foo\": response[\"bar\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab7c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P167 :Multi-agents: supervisor architecture\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class SupervisorDecision(State):\n",
    "    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0)\n",
    "model = model.with_structured_output(SupervisorDecision)\n",
    "\n",
    "agents = [\"researcher\", \"coder\"]\n",
    "\n",
    "system_prompt_part1 = SystemMessage(\"You are a supervisor tasked to manage conversation between these agens: {agents}. given the following user query, respond with worker\\\n",
    "                                    to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.\")\n",
    "system_prompt_part2 = SystemMessage(f\"Given the conversation above, who should act next? Or should we FINISH? Select one of : {', '.join(agents)}, FINISH\")\n",
    "\n",
    "def supervisor(state):\n",
    "    messages = [(\"system\", system_prompt_part1),\n",
    "                *state[\"messages\"],\n",
    "                (\"system\", system_prompt_part2)]\n",
    "    return model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533a0b9",
   "metadata": {},
   "source": [
    "# Chapter 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546fe49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band? Because it wanted to be the purr-cussionist. ', punchline='I hope you found that one paws-itively hilarious!')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P174: structured output\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str =Field(description=\"The punchline to the joke\")\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0.5)\n",
    "model = model.with_structured_output(Joke)\n",
    "\n",
    "model.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ffb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " cat\n",
      " join\n",
      " a\n",
      " band\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " it\n",
      " wanted\n",
      " to\n",
      " be\n",
      " the\n",
      " pur\n",
      "r\n",
      "-c\n",
      "ussion\n",
      "ist\n",
      "!\n",
      " (\n",
      "get\n",
      " it\n",
      "?)\n"
     ]
    }
   ],
   "source": [
    "# P178: Streaming LLM output token-by-token\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0.3)\n",
    "\n",
    "output = model.astream_events(\"Tell me a joke about cats\")\n",
    "\n",
    "async for event in output:\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        content=event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c779346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
