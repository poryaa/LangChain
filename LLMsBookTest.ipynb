{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b8e4b0",
   "metadata": {},
   "source": [
    "# Review and rerun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43acd53",
   "metadata": {},
   "source": [
    "## Chapter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59874a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama without langchain\n",
    "import ollama\n",
    "result = ollama.generate(model=\"gemma3:1b\", prompt=\"why is the sly blue?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama inside the langchain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "llm.invoke(\"the sky is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama as chatbot\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "message = [{\"role\": \"system\", \"content\":\"You are a helpful translator. Translate the user sentence to german.\"},\n",
    "           {\"role\": \"user\", \"content\":\"I love learing french.\"}]\n",
    "\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f63aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "                                             (\"human\", \"{input}\")])\n",
    "\n",
    "chain = template | llm\n",
    "chain.invoke({\"input_language\":\"English\",\n",
    "              \"output_language\": \"German\",\n",
    "              \"input\": \"I love programming.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6951cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p8)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_template(\"\"\"You are a translator. translate the below Context:\".\n",
    "Context: {context}\n",
    "Language: {language}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p11)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    justification: str\n",
    "    '''justification for the answer'''\n",
    "    \n",
    "llm = ChatOllama(model='llama3.1')\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    '''An answer to the user question along with number of words for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    num_of_words: int\n",
    "    '''number of words for the answer'''\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\").with_structured_output(Answer)\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming and teaching.\",\n",
    "        \"language\": \"german\"})\n",
    "\n",
    "chain = template | model\n",
    "response =chain.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using runnable interface(invoke(), bathc(), stream())\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "#1 invoke()\n",
    "#llm.invoke(\"hello there!\")\n",
    "\n",
    "#2 batch()\n",
    "#llm.batch([\"hi there!\", \"what is your name?\"])\n",
    "\n",
    "#3 stream()\n",
    "# for i in llm.stream('Bye!'):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imperative Composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f7380",
   "metadata": {},
   "source": [
    "## Chapter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path=\"./test.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now splitting it to a number of chunks\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "splitted_docs = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd66f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length: \", len(splitted_docs), end='\\n\\n'),\n",
    "print(\"Page content: \", splitted_docs[100].page_content, end='\\n\\n')\n",
    "print(\"All metadata: \", splitted_docs[0].metadata, end='\\n\\n')\n",
    "print(\"An example of metadata: \", splitted_docs[0].metadata[\"page_label\"], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc60357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking when we have only raw data not a doc\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "text = (\"horse is for king\")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "horse = embedding_model.embed_documents(text)\n",
    "# len(vector[0])\n",
    "# vector[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use document loade, splitting text and embedding all combined\n",
    "#1- load the file\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#2- splitt the documnet\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#3- embedding the documnet\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "embeddings = embedding_model.embed_documents([chunk.page_content for chunk in splitted_docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splitted_docs), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for work labtop\n",
    "# docker run --name pgvector-container \\\n",
    "# -e POSTGRES_USER=langchain \\\n",
    "# -e POSTGRES_PASSWORD=langchain \\\n",
    "# -e POSTGRES_DB=langchain \\\n",
    "# -e POSTGRES_HOST_AUTH_METHOD=md5 \\\n",
    "# -p 6024:5432 \\\n",
    "# -v pgvector-data:/var/lib/postgresql/data \\\n",
    "# -d pgvector/pgvector:pg16\n",
    "\n",
    "# connection_string:\"d539c9988113e8335cb996537db3cbcaf12438bece7430b9e11da4a311b37bc0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGVector\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#split the documnet\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter()             #chunk_size=1000, chunk_overlap=100\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#embedding the documnet\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model=\"tinyllama:latest\")\n",
    "\n",
    "db = PGVector.from_documents(splitted_docs, embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e43fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the similarity (by using model embedding only)\n",
    "db.similarity_search(\"Ahrar Institute of Technology and Higher\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to add new data to the db\n",
    "import uuid\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "\n",
    "db.add_documents([\n",
    "                    Document(page_content=\"there are cats in the pond\",\n",
    "                            metadata={\"location\":\"pond\" ,\"topic\":\"animals\"}),\n",
    "                    Document(page_content=\"Ducks are also found in the pond\",\n",
    "                    metadata={\"location\":\"pond\" ,\"topic\":\"animals\"})],\n",
    "    ids=ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52708b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"are cats in the pond\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed736c65",
   "metadata": {},
   "source": [
    "## Chapter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p62 retrieve data\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#fetch relevant documents\n",
    "docs = retriever.invoke(\"\"\"What are the main components and specifications of the experimental fault simulator setup\n",
    "                         at Ahrar Institute of Technology and Higher Education (AITHE),\n",
    "                         and how were vibration signals collected and measured for different fault conditions?\"\"\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p64 retrieve data from db and run llm\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer the question only based on the following context: {context}, Question:{question}.\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "chain = prompt | llm\n",
    "#fetch relevant documents\n",
    "question= \"\"\" which institute were the experimental recordings in this setup obtained?\"\"\"\n",
    "docs = retriever.invoke(question, k=5)\n",
    "\n",
    "#run\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p65 encapsulate the retrieval logic\n",
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question only based on th following context:\n",
    "                                          context: {context},\n",
    "                                          question: {question}.\"\"\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    #fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    #format prompt\n",
    "    formatted_prompt = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "    return answer\n",
    "\n",
    "#run\n",
    "qa.invoke(\"\"\"What role does the frequency of intermittent impulses in a vibration signal play\n",
    "           in detecting bearing faults and identifying the location of defects within bearing components?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p74: RAG fusion for query transformation\n",
    "#part1\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant that generates multiple search queries\n",
    "                                                      based on a single input query. \\n\n",
    "                                                     generate multiple search queries related to: {question} \\n\n",
    "                                                     Output (4 queries): \"\"\")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen.invoke(\"which institute were the experimental recordings in this setup obtained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents and an optional parameter k used in the RRF formula\"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d32509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "rag_fusion.invoke(\"In which institute were the dataset recorded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p78: Hypothetical Document embedding\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to answer the question.\\n\n",
    "                                                Question: {question}\\n\n",
    "                                                Passage:\"\"\")\n",
    "\n",
    "generate_doc = (prompt_hyde | llm | StrOutputParser())\n",
    "\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "query = \"where the experimental set of the faults simulator in the rotating machines were recorded?\"\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bd117",
   "metadata": {},
   "source": [
    "# Chapter04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p96: store and use all previous messages for chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability\"),\n",
    "                                           (\"placeholder\",\"{messages}\")])\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"messages\":[\"human\",\"Translate this from English to German: I love programming.\",\n",
    "                          \"ai\",\"Ich liebe programmieren.\",\n",
    "                          \"human\",\"what did you say?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982de3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p101: Simple LangGraph for chatbot\n",
    "\n",
    "#part1: the langgraph itself\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START,END,StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aad33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2: the chatbot node\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2362b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3: add edges\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part4: run the graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "input = {\"messages\": [HumanMessage(\"hi!\")]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p105: adding memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "graph = builder.compile(checkpointer = MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d030996",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = {\"configurable\": {\"thread_id\":\"1\"}}\n",
    "result_1 = graph.invoke({\"messages\": [HumanMessage(\"hi my name is XXX.\")]},\n",
    "                        thread1)\n",
    "result_2 = graph.invoke({\"messages\": [HumanMessage(\"what is my name?\")]},\n",
    "                        thread1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f787e",
   "metadata": {},
   "source": [
    "# Chapter05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ed48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P118: Arcitecture#1: LLM Call\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph=builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "input={\"messages\": [HumanMessage('hi!')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p121: Arcitecture#2: Chain\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model_1 = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)#low temp to generate SQL query\n",
    "model_2 = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)#high temp to generate natural language outputs\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]# to track the state and conversations\n",
    "    user_query: str #input\n",
    "    sql_query: str #output\n",
    "    sql_explanation: str #output\n",
    "\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    sql_query: str\n",
    "    sql_explanation: str\n",
    "############generation part################\n",
    "generate_prompt = SystemMessage('''You are a helpful data analyst who generates SQL queries for users based on their questions.''' )\n",
    "def generate_sql(state: State):\n",
    "    user_msg = HumanMessage(state[\"user_query\"])\n",
    "    messages = [generate_prompt, *state[\"messages\"], user_msg]\n",
    "    res = model_1.invoke(messages)\n",
    "    return{\n",
    "        \"sql_query\": res.content,\n",
    "        \"messages\": [user_msg, res]# update converstion history\n",
    "    }\n",
    "############explanantion part################\n",
    "explain_prompt = SystemMessage('''You are a helpful data analyst who explains SQL queries to users''')\n",
    "def explain_sql(state: State):\n",
    "    messages = [explain_prompt, *state[\"messages\"]]\n",
    "    res = model_2.invoke(messages)\n",
    "    return{\n",
    "        \"sql_explanation\": res.content,\n",
    "        \"messages\": [*state[\"messages\"], res] # update converstion history\n",
    "    }\n",
    "############build LangGraph################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"generate_sql\", generate_sql)\n",
    "builder.add_node(\"explain_sql\", explain_sql)\n",
    "builder.add_edge(START,\"generate_sql\")\n",
    "builder.add_edge(\"generate_sql\", \"explain_sql\")\n",
    "builder.add_edge(\"explain_sql\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "graph.invoke({\"user_query\": \"What is the total sales for each product?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P126: Arcitecture#3: Router\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from typing import Annotated, TypedDict,Literal\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "model_low_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)\n",
    "model_high_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)\n",
    "\n",
    "############ State prepration ################\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_query: str\n",
    "    domain: Literal[\"records\", \"insurance\"]\n",
    "    documents: list[Document]\n",
    "    answer: str\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    documents: list[Document]\n",
    "\n",
    "############ Vectore store prepration ################\n",
    "medical_records_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "medical_records_retriever = medical_records_store.as_retriever()\n",
    "\n",
    "insurance_faqs_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "insurance_faqs_retriever = insurance_faqs_store.as_retriever()\n",
    "\n",
    "############ Router part ################\n",
    "router_prompt = SystemMessage(\n",
    "    \"\"\"You need to decide which domain to route the user query to. You have two domains to choose from:\n",
    "- records: contains medical records of the patient, such as diagnosis, treatment, and prescriptions.\n",
    "- insurance: contains frequently asked questions about insurance policies, claims, and coverage.\n",
    "\n",
    "Output only the domain name.\"\"\")\n",
    "def router_node(state: State) -> State:\n",
    "    user_message = HumanMessage(state[\"user_query\"])\n",
    "    messages = [router_prompt, *state[\"messages\"], user_message]\n",
    "    res = model_low_temp.invoke(messages)\n",
    "    return {\n",
    "        \"domain\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": [user_message, res],\n",
    "    }\n",
    "############ pick part ################\n",
    "def pick_retriever(\n",
    "    state: State,\n",
    ") -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        return \"retrieve_medical_records\"\n",
    "    else:\n",
    "        return \"retrieve_insurance_faqs\"\n",
    "    \n",
    "def retrieve_medical_records(state: State) -> State:\n",
    "    documents = medical_records_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_insurance_faqs(state: State) -> State:\n",
    "    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "############ retrieve part ################    \n",
    "medical_records_prompt = SystemMessage(\n",
    "    \"You are a helpful medical chatbot, who answers questions based on the patient's medical records, such as diagnosis, treatment, and prescriptions.\"\n",
    ")\n",
    "\n",
    "insurance_faqs_prompt = SystemMessage(\n",
    "    \"You are a helpful medical insurance chatbot, who answers frequently asked questions about insurance policies, claims, and coverage.\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: State) -> State:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        prompt = medical_records_prompt\n",
    "    else:\n",
    "        prompt = insurance_faqs_prompt\n",
    "    messages = [\n",
    "        prompt,\n",
    "        *state[\"messages\"],\n",
    "        HumanMessage(f\"Documents: {state['documents']}\"),\n",
    "    ]\n",
    "    res = model_high_temp.invoke(messages)\n",
    "    return {\n",
    "        \"answer\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": res,\n",
    "    }\n",
    "############ langgraph part ################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"router\", router_node)\n",
    "builder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\n",
    "builder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\"router\", pick_retriever)\n",
    "builder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\n",
    "builder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2d26c",
   "metadata": {},
   "source": [
    "# Chapter06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p139: Basic Agent architecture using a chat model and LangGraph\n",
    "\n",
    "#pip install duckduckgo-search\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a515cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chunk[\"chat_model\"][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56050025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P143: Always calling a tool first\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "def first_model(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_tool_call = ToolCall(name= \"DuckDuckGoSearchRun\", args={\"query\": query}, id = uuid4().hex)\n",
    "    return {\"messages\": AIMessage(content=\"\", tool_calls=[search_tool_call])}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"first_model\", first_model)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"first_model\")\n",
    "builder.add_edge(\"first_model\",\"tools\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a36057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P148: Dealing with many tools\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "llm_model =ChatOllama(model=\"llama3.1:latest\", temperature=0.1) \n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents([Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
    "                                                     embedding=embeddings_model).as_retriever()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state[\"selected_tools\"]]\n",
    "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "def selected_tools(state:State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return{'selected_tools': [doc.metadata[\"name\"] for doc  in tool_docs]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"select_tools\", selected_tools)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "builder.add_edge(\"select_tools\",\"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fe8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12777a22",
   "metadata": {},
   "source": [
    "# Chapter07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf23f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#P156: Reflection for agents\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseModel], add_messages]\n",
    "\n",
    "generate_prompt = SystemMessage(\"\"\"You are an essay assistant tasked with writing excellent 3-paragraph essays.\"\"\"\n",
    "                                \"\"\"Generate the best essay possible for user's request.\"\"\"\n",
    "                                \"\"\"if the user provide critique, respond with a revised version of your previous attempts.\"\"\")\n",
    "def generate(state:State)->State:\n",
    "    answer=llm_model.invoke([generate_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "reflection_prompt = SystemMessage(\"\"\"You are a teacher grading an essay submission. generate critique and recommendations for the user's submission.\"\"\"\n",
    "                                  \"\"\"provide detailed recommendations, including requests for length, depth, style, etc.\"\"\")\n",
    "def reflect(state:State)->State:\n",
    "    cls_map = {AIMessage: HumanMessage, HumanMessage: AIMessage}\n",
    "    translated = [reflection_prompt, state[\"messages\"][0]] + [cls_map[msg.__class__](content=msg.content) for msg in state[\"messages\"][1:]]\n",
    "    answer = llm_model.invoke(translated)\n",
    "    return {\"messages\": [HumanMessage(content=answer.content)]}\n",
    "\n",
    "def should_continue(state:State):\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        return END\n",
    "    else:\n",
    "        return \"reflect\"\n",
    "    \n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_node(\"reflect\", reflect)\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_conditional_edges(\"generate\", should_continue, {\"reflect\":\"reflect\", END: END})\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "graph = builder.compile() \n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New message: Here is a three-paragraph essay on the relevance of \"The Little Prince\" today:\n",
      "\n",
      "Antoine de Saint-Exupry's timeless novella, \"The Little Prince\", has been a beloved classic for generations since its p ...\n",
      "\n",
      "New message: Here's my critique and recommendations for your essay submission:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. You've successfully identified the enduring appeal of \"The Little Prince\" and highlighted its relevance to contemp ...\n",
      "\n",
      "New message: Thank you so much for your thoughtful critique! I appreciate the detailed feedback and recommendations. Here's my revised essay, which addresses the weaknesses and areas for improvement:\n",
      "\n",
      "**Revised Es ...\n",
      "\n",
      "New message: **Excellent revisions!**\n",
      "\n",
      "You've successfully addressed the weaknesses and areas for improvement I mentioned earlier. Here's a breakdown of what you've done well:\n",
      "\n",
      "* **Length:** You've met the 750-800 ...\n",
      "\n",
      "New message: Thank you so much for your detailed feedback on my revised essay! I'm thrilled to hear that I was able to address the weaknesses and areas for improvement you mentioned earlier.\n",
      "\n",
      "I'll definitely take  ...\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Write an essay about the relevance of 'The Little Prince' today.\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "for output in graph.stream(initial_state):\n",
    "    message_type = \"generate\" if \"generate\" in output else \"reflect\"\n",
    "    print(\"\\nNew message:\", output[message_type]\n",
    "          [\"messages\"][-1].content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61239747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
