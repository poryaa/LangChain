{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b8e4b0",
   "metadata": {},
   "source": [
    "# Review and rerun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43acd53",
   "metadata": {},
   "source": [
    "## Chapter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59874a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateResponse(model='gemma3:1b', created_at='2026-01-29T08:45:25.18820993Z', done=True, done_reason='stop', total_duration=3651588919, load_duration=784919233, prompt_eval_count=15, prompt_eval_duration=11201569, eval_count=693, eval_duration=2563388061, response='This is a fantastic question that dives into the world of internet culture and a really intriguing meme! The \"Sly Blue\" meme originated from a single image posted on Reddit in 2018. Here\\'s the breakdown of why it became so popular:\\n\\n**The Origin - A Simple Image**\\n\\nThe image itself is a close-up of a blueberry – a single, vibrant blue blueberry – in a way that\\'s almost unnaturally perfect. It\\'s oddly symmetrical and framed in a way that emphasizes the color.\\n\\n**The Meme\\'s Evolution**\\n\\nHere’s how it went viral:\\n\\n1. **The Initial Reaction:** People initially saw the image as a slightly unsettling, almost beautiful, anomaly. It was a visual anomaly – a perfectly-shaped blueberry - in a world of increasingly bizarre imagery.\\n\\n2. **The “Sly Blue” Explanation:** The real breakthrough came from a user named @TheBlueOne who explained that the image was inspired by a series of very specific, and bizarre, internet art pieces – primarily by the artist known as \"Sly Blue.\" These pieces often involved intricate, repeating patterns and elements, and the image\\'s symmetrical nature was key.\\n\\n3. **The \"Sly Blue\" Theme:** The \"Sly Blue\" is essentially a collection of these small, bizarre visuals.  Each image is a fragment of this overarching theme.  They\\'re usually abstract, pixelated, and designed to evoke a feeling of unease and intrigue.\\n\\n4. **The Meme\\'s Growth:**  People started creating memes based on the \"Sly Blue\" imagery. These memes often involved:\\n   * **Replicating the patterns:**  Users would copy and paste the various small images to create a visual puzzle.\\n   * **Adding text:**  The text in these memes usually consisted of phrases like \"Sly Blue,\" \"Follow the pattern,\" \"Blue is the answer,\" or variations of these.\\n\\n**Why it’s So Engaging**\\n\\n* **The Absurdity:** The image itself is undeniably strange and slightly unsettling.\\n* **The Mystery:**  The \"Sly Blue\" theme is deliberately vague and encourages speculation.\\n* **The Puzzle:** The meme format – the repeating patterns – creates a puzzle for people to solve.\\n* **Nostalgia:** It taps into a sense of internet nostalgia and a bit of dark humor.\\n\\n**In short, the \"Sly Blue\" meme is a fantastic example of how a seemingly simple image can spark a massive internet phenomenon through a carefully crafted concept and a whole lot of creativity.**\\n\\n**Resources for Further Viewing:**\\n\\n* **Reddit:** [https://www.reddit.com/r/memes/comments/5583c0/sly_blue_meme_analysis/](https://www.reddit.com/r/memes/comments/5583c0/sly_blue_meme_analysis/)\\n* **The Blue One\\'s Website:** [https://www.theblueone.com/](https://www.theblueone.com/) (This is where the \\'Sly Blue\\' artwork originated.)\\n\\nDo you have any specific aspects of the \"Sly Blue\" meme you’d like to know more about (e.g., the artwork, the evolution of the meme)?', thinking=None, context=[105, 2364, 107, 36425, 563, 506, 130246, 3730, 236881, 106, 107, 105, 4368, 107, 2094, 563, 496, 14853, 2934, 600, 120393, 1131, 506, 1902, 529, 8379, 6540, 532, 496, 2126, 50612, 40741, 236888, 669, 623, 236773, 586, 9595, 236775, 40741, 43864, 699, 496, 3161, 2471, 12551, 580, 76891, 528, 236743, 236778, 236771, 236770, 236828, 236761, 5715, 236789, 236751, 506, 25890, 529, 3217, 625, 5452, 834, 4913, 236787, 108, 1018, 818, 45769, 753, 562, 17983, 9466, 1018, 108, 818, 2471, 4850, 563, 496, 3107, 236772, 1048, 529, 496, 72290, 1271, 496, 3161, 236764, 28239, 3730, 72290, 1271, 528, 496, 1595, 600, 236789, 236751, 4180, 107148, 87822, 4011, 236761, 1030, 236789, 236751, 123816, 61081, 532, 40542, 528, 496, 1595, 600, 58811, 506, 2258, 236761, 108, 1018, 818, 205203, 236789, 236751, 36630, 1018, 108, 8291, 236858, 236751, 1217, 625, 3939, 23637, 236787, 108, 236770, 236761, 5213, 818, 20879, 54188, 53121, 9432, 14877, 5004, 506, 2471, 618, 496, 8427, 137806, 236764, 4180, 4148, 236764, 52648, 236761, 1030, 691, 496, 7426, 52648, 1271, 496, 13275, 236772, 24850, 72290, 753, 528, 496, 1902, 529, 15747, 50875, 41001, 236761, 108, 236778, 236761, 5213, 818, 999, 236773, 586, 9595, 236919, 107182, 53121, 669, 1759, 46341, 3588, 699, 496, 2430, 7489, 1392, 818, 16520, 4906, 1015, 9696, 600, 506, 2471, 691, 13696, 684, 496, 3605, 529, 1401, 3530, 236764, 532, 50875, 236764, 8379, 1610, 9097, 1271, 13336, 684, 506, 8325, 3224, 618, 623, 236773, 586, 9595, 1781, 3143, 9097, 3187, 5418, 52996, 236764, 43064, 9935, 532, 4820, 236764, 532, 506, 2471, 236789, 236751, 61081, 4135, 691, 2307, 236761, 108, 236800, 236761, 5213, 818, 623, 236773, 586, 9595, 236775, 21657, 53121, 669, 623, 236773, 586, 9595, 236775, 563, 13981, 496, 5249, 529, 1239, 1944, 236764, 50875, 62325, 236761, 138, 7795, 2471, 563, 496, 16975, 529, 672, 116355, 7824, 236761, 138, 7634, 236789, 500, 4781, 7628, 236764, 22308, 774, 236764, 532, 5402, 531, 104184, 496, 8178, 529, 3037, 781, 532, 114597, 236761, 108, 236812, 236761, 5213, 818, 205203, 236789, 236751, 26608, 53121, 138, 18370, 3931, 7107, 55537, 2721, 580, 506, 623, 236773, 586, 9595, 236775, 41001, 236761, 3143, 55537, 3187, 5418, 236787, 107, 139, 236829, 5213, 1479, 2766, 1194, 506, 9935, 53121, 138, 13465, 1093, 4865, 532, 21836, 506, 3572, 1944, 4876, 531, 2619, 496, 7426, 29344, 236761, 107, 139, 236829, 5213, 51613, 1816, 53121, 138, 818, 1816, 528, 1239, 55537, 4781, 31590, 529, 37485, 1133, 623, 236773, 586, 9595, 2098, 623, 27447, 506, 3759, 2098, 623, 16520, 563, 506, 3890, 2098, 653, 15936, 529, 1239, 236761, 108, 1018, 11355, 625, 236858, 236751, 1593, 203926, 1018, 108, 236829, 5213, 818, 16533, 25463, 665, 53121, 669, 2471, 4850, 563, 164774, 17163, 532, 8427, 137806, 236761, 107, 236829, 5213, 818, 72289, 53121, 138, 818, 623, 236773, 586, 9595, 236775, 7824, 563, 44217, 46020, 532, 37143, 42086, 236761, 107, 236829, 5213, 818, 84150, 53121, 669, 40741, 6518, 1271, 506, 43064, 9935, 1271, 14004, 496, 29344, 573, 1331, 531, 8974, 236761, 107, 236829, 5213, 236797, 731, 97334, 53121, 1030, 70541, 1131, 496, 5113, 529, 8379, 72441, 532, 496, 3103, 529, 4996, 29024, 236761, 108, 1018, 902, 2822, 236764, 506, 623, 236773, 586, 9595, 236775, 40741, 563, 496, 14853, 2591, 529, 1217, 496, 30043, 3606, 2471, 740, 14838, 496, 12566, 8379, 20284, 1343, 496, 13058, 43374, 3495, 532, 496, 3697, 2606, 529, 24597, 99382, 108, 1018, 19213, 573, 9077, 155712, 53121, 108, 236829, 5213, 214950, 53121, 870, 2574, 1411, 2769, 236761, 44183, 236761, 854, 236786, 236750, 236786, 8182, 507, 236786, 26329, 236786, 236810, 236810, 236828, 236800, 236755, 236771, 236786, 236751, 586, 236779, 9503, 236779, 185124, 236779, 28263, 236786, 5457, 2574, 1411, 2769, 236761, 44183, 236761, 854, 236786, 236750, 236786, 8182, 507, 236786, 26329, 236786, 236810, 236810, 236828, 236800, 236755, 236771, 236786, 236751, 586, 236779, 9503, 236779, 185124, 236779, 28263, 31004, 107, 236829, 5213, 818, 9595, 3656, 236789, 236751, 21124, 53121, 870, 2574, 1411, 2769, 236761, 1437, 9503, 811, 236761, 854, 236786, 5457, 2574, 1411, 2769, 236761, 1437, 9503, 811, 236761, 854, 31004, 568, 2094, 563, 1298, 506, 756, 236773, 586, 9595, 236789, 20431, 43864, 2907, 108, 6294, 611, 735, 1027, 3530, 10217, 529, 506, 623, 236773, 586, 9595, 236775, 40741, 611, 236858, 236753, 1133, 531, 1281, 919, 1003, 568, 236744, 236761, 236759, 1126, 506, 20431, 236764, 506, 11294, 529, 506, 40741, 20625])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call ollama without langchain\n",
    "import ollama\n",
    "result = ollama.generate(model=\"gemma3:1b\", prompt=\"why is the sly blue?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call ollama inside the langchain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "llm.invoke(\"the sky is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbca4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The most natural and common translation of “I love learning French” in German is:\\n\\n**Ich liebe es, Französisch zu lernen.**\\n\\nHere are a few other slightly nuanced options, depending on the emphasis you want to give:\\n\\n* **Ich mag es, Französisch zu lernen.** (This is a bit more polite and less enthusiastic than \"Ich liebe...\")\\n* **Ich liebe das Lernen von Französisch.** (This emphasizes the *act* of learning.)\\n\\n\\nBut **Ich liebe es, Französisch zu lernen.** is the best and most direct translation.\\n\\nDo you want me to translate something else?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2026-01-29T10:50:51.454731978Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1421025479, 'load_duration': 814893753, 'prompt_eval_count': 33, 'prompt_eval_duration': 12052433, 'eval_count': 130, 'eval_duration': 524208180, 'model_name': 'gemma3:1b'}, id='run--67c8b755-0de6-4000-8af3-77b527319f28-0', usage_metadata={'input_tokens': 33, 'output_tokens': 130, 'total_tokens': 163})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call ollama as chatbot\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "message = [{\"role\": \"system\", \"content\":\"You are a helpful translator. Translate the user sentence to german.\"},\n",
    "           {\"role\": \"user\", \"content\":\"I love learing french.\"}]\n",
    "\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f63aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Programmieren liebe ich.\" \\n\\nWould you like me to translate anything else?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2026-01-29T11:00:34.663787609Z', 'done': True, 'done_reason': 'stop', 'total_duration': 390080822, 'load_duration': 64105452, 'prompt_eval_count': 30, 'prompt_eval_duration': 36853693, 'eval_count': 18, 'eval_duration': 277169505, 'model_name': 'llama3.1'}, id='run--14a8ccad-8e1b-460b-8724-f338b03c447b-0', usage_metadata={'input_tokens': 30, 'output_tokens': 18, 'total_tokens': 48})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat with promt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "                                             (\"human\", \"{input}\")])\n",
    "\n",
    "chain = template | llm\n",
    "chain.invoke({\"input_language\":\"English\",\n",
    "              \"output_language\": \"German\",\n",
    "              \"input\": \"I love programming.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6951cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p8)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_template(\"\"\"You are a translator. translate the below Context:\".\n",
    "Context: {context}\n",
    "Language: {language}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat with promt template(book example p11)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\")\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming.\",\n",
    "        \"language\": \"german\"})\n",
    "chain = template | llm\n",
    "response =chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005c4ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same!', justification=\"A pound is a unit of weight, so it's equal to a certain amount of mass. Therefore, no matter what you're weighing (bricks, feathers, water, etc.), if it weighs one pound, it has that exact same amount of mass as something else that also weighs one pound. That means they weigh the same!\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    justification: str\n",
    "    '''justification for the answer'''\n",
    "    \n",
    "llm = ChatOllama(model='llama3.1')\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =ChatPromptTemplate.from_messages([(\"system\", \"You are a translator. translate the below Context:\"),\n",
    "                                            (\"human\", \"text context: {context}\"),\n",
    "                                            (\"human\", \"desired language: {language}\")])\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    '''An answer to the user question along with number of words for the answer.'''\n",
    "    answer:str\n",
    "    '''the answer to the user question'''\n",
    "    num_of_words: int\n",
    "    '''number of words for the answer'''\n",
    "\n",
    "model = ChatOllama(model= \"llama3.1\").with_structured_output(Answer)\n",
    "\n",
    "prompt = ({\n",
    "        \"context\": \"I love programming and teaching.\",\n",
    "        \"language\": \"german\"})\n",
    "\n",
    "chain = template | model\n",
    "response =chain.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using runnable interface(invoke(), bathc(), stream())\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "#1 invoke()\n",
    "#llm.invoke(\"hello there!\")\n",
    "\n",
    "#2 batch()\n",
    "#llm.batch([\"hi there!\", \"what is your name?\"])\n",
    "\n",
    "#3 stream()\n",
    "# for i in llm.stream('Bye!'):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imperative Composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f7380",
   "metadata": {},
   "source": [
    "## Chapter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(file_path=\"./test.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now splitting it to a number of chunks\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
    "splitted_docs = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd66f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length: \", len(splitted_docs), end='\\n\\n'),\n",
    "print(\"Page content: \", splitted_docs[100].page_content, end='\\n\\n')\n",
    "print(\"All metadata: \", splitted_docs[0].metadata, end='\\n\\n')\n",
    "print(\"An example of metadata: \", splitted_docs[0].metadata[\"page_label\"], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc60357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking when we have only raw data not a doc\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "text = (\"horse is for king\")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "horse = embedding_model.embed_documents(text)\n",
    "# len(vector[0])\n",
    "# vector[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use document loade, splitting text and embedding all combined\n",
    "#1- load the file\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#2- splitt the documnet\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#3- embedding the documnet\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n",
    "embeddings = embedding_model.embed_documents([chunk.page_content for chunk in splitted_docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splitted_docs), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for work labtop\n",
    "# docker run --name pgvector-container \\\n",
    "# -e POSTGRES_USER=langchain \\\n",
    "# -e POSTGRES_PASSWORD=langchain \\\n",
    "# -e POSTGRES_DB=langchain \\\n",
    "# -e POSTGRES_HOST_AUTH_METHOD=md5 \\\n",
    "# -p 6024:5432 \\\n",
    "# -v pgvector-data:/var/lib/postgresql/data \\\n",
    "# -d pgvector/pgvector:pg16\n",
    "\n",
    "# connection_string:\"d539c9988113e8335cb996537db3cbcaf12438bece7430b9e11da4a311b37bc0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGVector\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "document_loader = PyPDFLoader(file_path=\"./test_article.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "#split the documnet\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter()             #chunk_size=1000, chunk_overlap=100\n",
    "splitted_docs = splitter.split_documents(document)\n",
    "\n",
    "#embedding the documnet\n",
    "connection = \"postgresql+psycopg://langchain:langchain@127.0.0.1:6024/langchain\"\n",
    "embedding_model = OllamaEmbeddings(model=\"tinyllama:latest\")\n",
    "\n",
    "db = PGVector.from_documents(splitted_docs, embedding_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e43fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the similarity (by using model embedding only)\n",
    "db.similarity_search(\"Ahrar Institute of Technology and Higher\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to add new data to the db\n",
    "import uuid\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "\n",
    "db.add_documents([\n",
    "                    Document(page_content=\"there are cats in the pond\",\n",
    "                            metadata={\"location\":\"pond\" ,\"topic\":\"animals\"}),\n",
    "                    Document(page_content=\"Ducks are also found in the pond\",\n",
    "                    metadata={\"location\":\"pond\" ,\"topic\":\"animals\"})],\n",
    "    ids=ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52708b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"are cats in the pond\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed736c65",
   "metadata": {},
   "source": [
    "## Chapter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p62 retrieve data\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#fetch relevant documents\n",
    "docs = retriever.invoke(\"\"\"What are the main components and specifications of the experimental fault simulator setup\n",
    "                         at Ahrar Institute of Technology and Higher Education (AITHE),\n",
    "                         and how were vibration signals collected and measured for different fault conditions?\"\"\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p64 retrieve data from db and run llm\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer the question only based on the following context: {context}, Question:{question}.\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "chain = prompt | llm\n",
    "#fetch relevant documents\n",
    "question= \"\"\" which institute were the experimental recordings in this setup obtained?\"\"\"\n",
    "docs = retriever.invoke(question, k=5)\n",
    "\n",
    "#run\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p65 encapsulate the retrieval logic\n",
    "from langchain_ollama import ChatOllama \n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question only based on th following context:\n",
    "                                          context: {context},\n",
    "                                          question: {question}.\"\"\")\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    #fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    #format prompt\n",
    "    formatted_prompt = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    #generate answer\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "    return answer\n",
    "\n",
    "#run\n",
    "qa.invoke(\"\"\"What role does the frequency of intermittent impulses in a vibration signal play\n",
    "           in detecting bearing faults and identifying the location of defects within bearing components?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p74: RAG fusion for query transformation\n",
    "#part1\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant that generates multiple search queries\n",
    "                                                      based on a single input query. \\n\n",
    "                                                     generate multiple search queries related to: {question} \\n\n",
    "                                                     Output (4 queries): \"\"\")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen.invoke(\"which institute were the experimental recordings in this setup obtained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents and an optional parameter k used in the RRF formula\"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d32509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3 \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\")\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "rag_fusion.invoke(\"In which institute were the dataset recorded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p78: Hypothetical Document embedding\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to answer the question.\\n\n",
    "                                                Question: {question}\\n\n",
    "                                                Passage:\"\"\")\n",
    "\n",
    "generate_doc = (prompt_hyde | llm | StrOutputParser())\n",
    "\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"tinyllama:latest\", temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "query = \"where the experimental set of the faults simulator in the rotating machines were recorded?\"\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bd117",
   "metadata": {},
   "source": [
    "# Chapter04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p96: store and use all previous messages for chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability\"),\n",
    "                                           (\"placeholder\",\"{messages}\")])\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"messages\":[\"human\",\"Translate this from English to German: I love programming.\",\n",
    "                          \"ai\",\"Ich liebe programmieren.\",\n",
    "                          \"human\",\"what did you say?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982de3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p101: Simple LangGraph for chatbot\n",
    "\n",
    "#part1: the langgraph itself\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START,END,StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aad33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2: the chatbot node\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2362b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3: add edges\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part4: run the graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "input = {\"messages\": [HumanMessage(\"hi!\")]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p105: adding memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "graph = builder.compile(checkpointer = MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d030996",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = {\"configurable\": {\"thread_id\":\"1\"}}\n",
    "result_1 = graph.invoke({\"messages\": [HumanMessage(\"hi my name is XXX.\")]},\n",
    "                        thread1)\n",
    "result_2 = graph.invoke({\"messages\": [HumanMessage(\"what is my name?\")]},\n",
    "                        thread1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f787e",
   "metadata": {},
   "source": [
    "# Chapter05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ed48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P118: Arcitecture#1: LLM Call\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\")\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph=builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "input={\"messages\": [HumanMessage('hi!')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p121: Arcitecture#2: Chain\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model_1 = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)#low temp to generate SQL query\n",
    "model_2 = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)#high temp to generate natural language outputs\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]# to track the state and conversations\n",
    "    user_query: str #input\n",
    "    sql_query: str #output\n",
    "    sql_explanation: str #output\n",
    "\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    sql_query: str\n",
    "    sql_explanation: str\n",
    "############generation part################\n",
    "generate_prompt = SystemMessage('''You are a helpful data analyst who generates SQL queries for users based on their questions.''' )\n",
    "def generate_sql(state: State):\n",
    "    user_msg = HumanMessage(state[\"user_query\"])\n",
    "    messages = [generate_prompt, *state[\"messages\"], user_msg]\n",
    "    res = model_1.invoke(messages)\n",
    "    return{\n",
    "        \"sql_query\": res.content,\n",
    "        \"messages\": [user_msg, res]# update converstion history\n",
    "    }\n",
    "############explanantion part################\n",
    "explain_prompt = SystemMessage('''You are a helpful data analyst who explains SQL queries to users''')\n",
    "def explain_sql(state: State):\n",
    "    messages = [explain_prompt, *state[\"messages\"]]\n",
    "    res = model_2.invoke(messages)\n",
    "    return{\n",
    "        \"sql_explanation\": res.content,\n",
    "        \"messages\": [*state[\"messages\"], res] # update converstion history\n",
    "    }\n",
    "############build LangGraph################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"generate_sql\", generate_sql)\n",
    "builder.add_node(\"explain_sql\", explain_sql)\n",
    "builder.add_edge(START,\"generate_sql\")\n",
    "builder.add_edge(\"generate_sql\", \"explain_sql\")\n",
    "builder.add_edge(\"explain_sql\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "graph.invoke({\"user_query\": \"What is the total sales for each product?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P126: Arcitecture#3: Router\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama\n",
    "from typing import Annotated, TypedDict,Literal\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "model_low_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.1)\n",
    "model_high_temp = ChatOllama(model=\"llama3.1:latest\", temperature=0.7)\n",
    "\n",
    "############ State prepration ################\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_query: str\n",
    "    domain: Literal[\"records\", \"insurance\"]\n",
    "    documents: list[Document]\n",
    "    answer: str\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "class Output(TypedDict):\n",
    "    documents: list[Document]\n",
    "\n",
    "############ Vectore store prepration ################\n",
    "medical_records_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "medical_records_retriever = medical_records_store.as_retriever()\n",
    "\n",
    "insurance_faqs_store = InMemoryVectorStore.from_documents([],embedding_model)\n",
    "insurance_faqs_retriever = insurance_faqs_store.as_retriever()\n",
    "\n",
    "############ Router part ################\n",
    "router_prompt = SystemMessage(\n",
    "    \"\"\"You need to decide which domain to route the user query to. You have two domains to choose from:\n",
    "- records: contains medical records of the patient, such as diagnosis, treatment, and prescriptions.\n",
    "- insurance: contains frequently asked questions about insurance policies, claims, and coverage.\n",
    "\n",
    "Output only the domain name.\"\"\")\n",
    "def router_node(state: State) -> State:\n",
    "    user_message = HumanMessage(state[\"user_query\"])\n",
    "    messages = [router_prompt, *state[\"messages\"], user_message]\n",
    "    res = model_low_temp.invoke(messages)\n",
    "    return {\n",
    "        \"domain\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": [user_message, res],\n",
    "    }\n",
    "############ pick part ################\n",
    "def pick_retriever(\n",
    "    state: State,\n",
    ") -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        return \"retrieve_medical_records\"\n",
    "    else:\n",
    "        return \"retrieve_insurance_faqs\"\n",
    "    \n",
    "def retrieve_medical_records(state: State) -> State:\n",
    "    documents = medical_records_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_insurance_faqs(state: State) -> State:\n",
    "    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "############ retrieve part ################    \n",
    "medical_records_prompt = SystemMessage(\n",
    "    \"You are a helpful medical chatbot, who answers questions based on the patient's medical records, such as diagnosis, treatment, and prescriptions.\"\n",
    ")\n",
    "\n",
    "insurance_faqs_prompt = SystemMessage(\n",
    "    \"You are a helpful medical insurance chatbot, who answers frequently asked questions about insurance policies, claims, and coverage.\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: State) -> State:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        prompt = medical_records_prompt\n",
    "    else:\n",
    "        prompt = insurance_faqs_prompt\n",
    "    messages = [\n",
    "        prompt,\n",
    "        *state[\"messages\"],\n",
    "        HumanMessage(f\"Documents: {state['documents']}\"),\n",
    "    ]\n",
    "    res = model_high_temp.invoke(messages)\n",
    "    return {\n",
    "        \"answer\": res.content,\n",
    "        # update conversation history\n",
    "        \"messages\": res,\n",
    "    }\n",
    "############ langgraph part ################\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"router\", router_node)\n",
    "builder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\n",
    "builder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\"router\", pick_retriever)\n",
    "builder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\n",
    "builder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Example usage\n",
    "input = {\"user_query\": \"Am I covered for COVID-19 treatment?\"}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2d26c",
   "metadata": {},
   "source": [
    "# Chapter06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p139: Basic Agent architecture using a chat model and LangGraph\n",
    "\n",
    "#pip install duckduckgo-search\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a515cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chunk[\"chat_model\"][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56050025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P143: Always calling a tool first\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\").bind_tools(tools)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "\n",
    "def first_model(state: State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_tool_call = ToolCall(name= \"DuckDuckGoSearchRun\", args={\"query\": query}, id = uuid4().hex)\n",
    "    return {\"messages\": AIMessage(content=\"\", tool_calls=[search_tool_call])}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"first_model\", first_model)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"first_model\")\n",
    "builder.add_edge(\"first_model\",\"tools\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a36057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P148: Dealing with many tools\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import ast\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolCall, AIMessage\n",
    "from uuid import uuid4\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@tool\n",
    "def calculator(query: str)->str:\n",
    "    \"\"\"A simple calculator. input should be a a mathematical expression.\"\"\"\n",
    "    return ast.literal_eval(query)\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "llm_model =ChatOllama(model=\"llama3.1:latest\", temperature=0.1) \n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents([Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\n",
    "                                                     embedding=embeddings_model).as_retriever()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state[\"selected_tools\"]]\n",
    "    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": res}\n",
    "def selected_tools(state:State) -> State:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return{'selected_tools': [doc.metadata[\"name\"] for doc  in tool_docs]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"select_tools\", selected_tools)\n",
    "builder.add_node(\"chat_model\", model_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "builder.add_edge(\"select_tools\",\"chat_model\")\n",
    "builder.add_conditional_edges(\"chat_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"chat_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fe8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(\"\"\"How old was the 30th president of the united states when he died?\"\"\")]}\n",
    "\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12777a22",
   "metadata": {},
   "source": [
    "# Chapter07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf23f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P156: Reflection for agents\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, TypedDict\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "llm_model = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseModel], add_messages]\n",
    "\n",
    "generate_prompt = SystemMessage(\"\"\"You are an essay assistant tasked with writing excellent 3-paragraph essays.\"\"\"\n",
    "                                \"\"\"Generate the best essay possible for user's request.\"\"\"\n",
    "                                \"\"\"if the user provide critique, respond with a revised version of your previous attempts.\"\"\")\n",
    "def generate(state:State)->State:\n",
    "    answer=llm_model.invoke([generate_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "reflection_prompt = SystemMessage(\"\"\"You are a teacher grading an essay submission. generate critique and recommendations for the user's submission.\"\"\"\n",
    "                                  \"\"\"provide detailed recommendations, including requests for length, depth, style, etc.\"\"\")\n",
    "def reflect(state:State)->State:\n",
    "    cls_map = {AIMessage: HumanMessage, HumanMessage: AIMessage}\n",
    "    translated = [reflection_prompt, state[\"messages\"][0]] + [cls_map[msg.__class__](content=msg.content) for msg in state[\"messages\"][1:]]\n",
    "    answer = llm_model.invoke(translated)\n",
    "    return {\"messages\": [HumanMessage(content=answer.content)]}\n",
    "\n",
    "def should_continue(state:State):\n",
    "    if len(state[\"messages\"]) > 6:\n",
    "        return END\n",
    "    else:\n",
    "        return \"reflect\"\n",
    "    \n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_node(\"reflect\", reflect)\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_conditional_edges(\"generate\", should_continue, {\"reflect\":\"reflect\", END: END})\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "graph = builder.compile() \n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Write an essay about the relevance of 'The Little Prince' today.\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "for output in graph.stream(initial_state):\n",
    "    message_type = \"generate\" if \"generate\" in output else \"reflect\"\n",
    "    print(\"\\nNew message:\", output[message_type]\n",
    "          [\"messages\"][-1].content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61239747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P162: method1:calling a Subgraph directly\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph \n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node(state:SubgraphState)->State:\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "# subgraph_builder = StateGraph(SubgraphState)\n",
    "# subgraph_builder.add_node(subgraph_node)\n",
    "# #...\n",
    "# subgragh = subgraph_builder.compile()\n",
    "\n",
    "# #Define parent graph\n",
    "# builder = StateGraph(State)\n",
    "# builder.add_node(\"subgraph\", subgragh)\n",
    "# # ...\n",
    "# graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P164: method2:calling a Subgraph with a function\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "class SubgraphState(TypedDict):\n",
    "    baz: str\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node(state:SubgraphState):\n",
    "    return {\"bar\": state[\"bar\"] + \"baz\"}\n",
    "\n",
    "# subgraph_builder = StateGraph(SubgraphState)\n",
    "# subgraph_builder.add_node(subgraph_node)\n",
    "# #...\n",
    "# subgragh = subgraph_builder.compile()\n",
    "\n",
    "# def node(state: State):\n",
    "#     response = subgraph.invoke({\"bar\": state[\"foo\"]})\n",
    "#     return {\"foo\": response[\"bar\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P167 :Multi-agents: supervisor architecture\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class SupervisorDecision(State):\n",
    "    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0)\n",
    "model = model.with_structured_output(SupervisorDecision)\n",
    "\n",
    "agents = [\"researcher\", \"coder\"]\n",
    "\n",
    "system_prompt_part1 = SystemMessage(\"You are a supervisor tasked to manage conversation between these agens: {agents}. given the following user query, respond with worker\\\n",
    "                                    to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.\")\n",
    "system_prompt_part2 = SystemMessage(f\"Given the conversation above, who should act next? Or should we FINISH? Select one of : {', '.join(agents)}, FINISH\")\n",
    "\n",
    "def supervisor(state):\n",
    "    messages = [(\"system\", system_prompt_part1),\n",
    "                *state[\"messages\"],\n",
    "                (\"system\", system_prompt_part2)]\n",
    "    return model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533a0b9",
   "metadata": {},
   "source": [
    "# Chapter 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fe49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P174: structured output\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str =Field(description=\"The punchline to the joke\")\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0.5)\n",
    "model = model.with_structured_output(Joke)\n",
    "\n",
    "model.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ffb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P178: Streaming LLM output token-by-token\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0.3)\n",
    "\n",
    "output = model.astream_events(\"Tell me a joke about cats\")\n",
    "\n",
    "async for event in output:\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        content=event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc356aa",
   "metadata": {},
   "source": [
    "# Chapter 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c779346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
