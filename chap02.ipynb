{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.txt\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b85728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8490ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.langchain.com/', 'title': 'LangChain', 'description': 'LangChain’s suite of products supports developers along each step of their development journey.', 'language': 'en'}, page_content=\"LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upThe platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production.Request a demoSee the docs\\n\\nLangChain products power top engineering teams, from startups to global enterprisesAccelerate agent development.Build faster with templates & a visual agent IDE. Reuse, configure, and combine agents to go further with less code.Ship reliable agents.Design agents that can handle sophisticated tasks with control. Add human-in-the-loop to steer and approve agent actions.Gain visibility & improve quality.See what’s happening - so you can quickly trace to root cause and debug issues. Evaluate your agent performance to improve over time.The Agent StackORCHESTRATION:Build agents with LangGraphControllable agent orchestration with built-in persistence to handle conversational history, memory, and agent-to-agent collaboration.INTEGRATIONS:Integrate components with LangChainIntegrate with the latest models, databases, and tools with no engineering overhead.EVALS\\xa0&\\xa0OBSERVABILITY:Gain visibility with LangSmithDebug poor-performing LLM app runs. Evaluate and observe agent performance at scale.DEPLOYMENT:Deploy &\\xa0manage with LangGraph PlatformDeploy and scale enterprise-grade agents with long-running workflows. Discover, reuse, and share agents across teams — and iterate faster with LangGraph Studio.CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access\\u2028to information and tools\\u2028in a compliant manner so they\\u2028can perform their best.Customer SupportImprove the speed & efficiency\\u2028of support teams that handle customer requests.ResearchSynthesize data, summarize sources & uncover insights faster than ever for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way.\\n\\n\\n\\n\\n\\nLangChain products are designed to be used independently or stack for multiplicative benefit. LangChainLangGraphFrameworksLangSmithLangGraph PlatformPlatformsFrameworksLangChainLangGraphPlatformsLangSmithLangGraph \\u2028PlatformSTACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0PlatformA full product suite for reliable agents and LLM appsLangChain's products work seamlessly together to provide an integrated solution for every step of the application development journey. When you use all LangChain products, you'll build better, get to production quicker, and grow visibility -- all with less set up and friction. LangChain provides the smoothest path to high quality agents.Orchestration:Integrations:Evals + Observability:Deployment:STACK 2: No framework +\\xa0LangSmithTrace\\xa0and evaluate any LLM appLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain's frameworks or not.Orchestration:Your choiceEvals + Observability:STACK 3:\\xa0Any agent framework +\\xa0LangGraph PlatformBuild agents any way you want, then deploy and scale with easeLangGraph Platform works with any agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments.Orchestration:Your choiceDeployment:Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\\n\\n\\nFinancial ServicesKlarna's AI assistant has reduced average customer query resolution time by 80%, powered by LangSmith and LangGraph\\n\\n\\nTransportationThis global logistics provider is saving 600 hours a day using an automated order system built on LangGraph and LangSmith\\n\\n\\nSecurityAs a leading cybersecurity firm with 40k+ customers, Trellix cut log parsing from days to minutes using LangGraph and LangSmith.\\n\\n\\nThe biggest developer community in GenAILearn alongside the 1M+ practitioners using our frameworks to push the industry forward.#1Downloaded agent framework100k+GitHub stars#1Downloaded agent framework600+IntegrationsReady to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangSmithLangGraphResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogExpertsPython DocsLangGraph LangSmithLangChainJS DocsLangGraphLangSmithLangChainCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8356c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da5a02d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-08-26T08:19:18+00:00', 'title': 'Unable to read text data file using TextLoader from langchain.document_loaders library because of encoding issue - Stack Overflow', 'moddate': '2025-08-26T08:19:18+00:00', 'source': '/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='Unable to read text data file using TextLoader from\\nlangchain.document_loaders library because of encoding issue\\n Asked 2 years, 1 month ago  Modified 4 months ago  27k timesViewed\\n4\\nMy end goal is to read the contents of a file and create a vectorstore of my data which I can query later.\\n langchain.embeddings.openai  OpenAIEmbeddings\\n langchain.text_splitter  CharacterTextSplitter\\n langchain.vectorstores  FAISS\\n langchain.document_loaders  TextLoader\\nloader = TextLoader( )\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=, chunk_overlap=)\\ndocs = text_splitter.split_documents(documents)\\nfrom import\\nfrom import\\nfrom import\\nfrom import\\n\"elon_musk.txt\"\\n1000 0 \\nIt looks like there is some issue with my data file and because of this, it is not able to read the contents of\\nmy file. Is it possible to load my file in utf-8 format? My assumption is with utf-8 encoding I should not face\\nthis issue.\\nFollowing is the error I am getting in my code:\\n---------------------------------------------------------------------------\\nUnicodeDecodeError                        Traceback (most recent call last)\\nFile ~\\\\anaconda3\\\\envs\\\\langchain-test\\\\lib\\\\site-packages\\\\langchain\\\\document_loaders\\\\text.py:41, in \\nTextLoader.load(self)\\n     40     with open(self.file_path, encoding=self.encoding) as f:\\n---> 41         text = f.read()\\n     42 except UnicodeDecodeError as e:\\nFile ~\\\\anaconda3\\\\envs\\\\langchain-test\\\\lib\\\\encodings\\\\cp1252.py:23, in \\nIncrementalDecoder.decode(self, input, final)\\n     22 def decode(self, input, final=False):\\n---> 23     return codecs.charmap_decode(input,self.errors,decoding_table)[0]\\nUnicodeDecodeError: \\'charmap\\' codec can\\'t decode byte 0x9d in position 1897: character maps to \\n<undefined>\\nThe above exception was the direct cause of the following exception:\\nRuntimeError                              Traceback (most recent call last)\\nCell In[1], line 8\\n      4 from langchain.document_loaders import TextLoader\\n      7 loader = TextLoader(\"elon_musk.txt\")\\n----> 8 documents = loader.load()\\n      9 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n     10 docs = text_splitter.split_documents(documents)\\nFile ~\\\\anaconda3\\\\envs\\\\langchain-test\\\\lib\\\\site-packages\\\\langchain\\\\document_loaders\\\\text.py:54, in \\nTextLoader.load(self)\\n     52                 continue\\n     53     else:\\n---> 54         raise RuntimeError(f\"Error loading {self.file_path}\") from e\\n     55 except Exception as e:\\n     56     raise RuntimeError(f\"Error loading {self.file_path}\") from e')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2f29a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7566f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1 The Caspian Sea: An Overview\n",
      "1.1 The Caspian Sea, the world's largest enclosed inland body of water, is a remarkable natural entity shared by five countries: Russia, Kazakhstan, Turkmenistan, Iran, and Azerbaijan. Lying on the border of Europe and Asia, it is technically a lake but is called a “sea” due to its size and saline water content. Its surface area ranges from about 371,000 to 392,600 square kilometers, making it larger than the combined areas of Germany or Japan and significantly larger than North America's Great Lakes. The Caspian stretches roughly 1,200 kilometers from north to south and 320 kilometers across at its widest point, with an average depth varying drastically between its shallow northern sector and deep south, reaching a maximum depth of over 1,000 meters.' metadata={'source': '/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.txt'}\n",
      "page_content='2Geography and Physical Structure\n",
      "2.1The Caspian Sea is divided into three distinct regions: the Northern, Middle, and Southern Caspian. The Northern Caspian is extremely shallow, with an average depth of only 4–6 meters (and accounts for less than 1% of total water volume), while the Middle and Southern parts are progressively deeper, with the south containing the greatest depths (up to 1,025 meters). The sea’s coastline measures about 7,000 kilometers, bordered by both arid and economically developed regions.\n",
      "\n",
      "2.2The Caspian sits about 27–28 meters below sea level and is a closed system, meaning it has no natural outflow; the water balance is maintained primarily through river inflow—dominated by the Volga River (accounting for about 80% of inflow), with additional contributions from the Kura-Araks, Ural, and Terek rivers. This endorheic nature means water only leaves via evaporation, making the Caspian very sensitive to climate and hydrological changes.' metadata={'source': '/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"/home/paminidigehsara/Desktop/Lnagchain_git/LangChain/test.txt\"\n",
    "loader = TextLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b8f8509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Pouriya'}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={'source': 'Pouriya'}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE],[{\"source\" : \"Pouriya\"}])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f5c0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "model = OllamaEmbeddings(model =\"llama3.1\")\n",
    "embedding = model.embed_documents([\n",
    "    \"Hi there!\",\n",
    "    \"oh, hi again\",\n",
    "    \"Hi, there!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21e1f2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91db83a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.012446607, -0.011286946, -0.010457791, -0.008585999, -0.028381279]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ce09bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.013670289, -0.011971503, 0.012126673, 0.005403437, 0.014939791]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[1][0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1bbad6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0035919873, -0.014793748, -0.0059636435, -0.0044142106, -0.027265158]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[2][0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8971f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, split and embed a txt file\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "file_path = \"./test.txt\"\n",
    "loader = TextLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model =\"tinyllama\")\n",
    "embedding = model.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d761050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity using NumPy: 0.8682431421244593\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([1, 2])\n",
    "B = np.array([3, 2])\n",
    "\n",
    "dot_product = np.dot(A, B)\n",
    "magnitude_A = np.linalg.norm(A)\n",
    "magnitude_B = np.linalg.norm(B)\n",
    "\n",
    "cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "print(f\"Cosine Similarity using NumPy: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d889dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTIO_STRING=\"70130169b3fb09b757caaa9d207e0da099c7608d23ae60fd9c6e73b5e5e077d5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0185a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "conncetion = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(texts, embeddings_model, connection=conncetion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1177ecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='9eb35f34-f09c-405f-ae09-d9ac808f350b', metadata={'topic': 'animals', 'location': 'pond'}, page_content='there are cats in the pond'),\n",
       " Document(id='47bff19e-58ef-4431-a048-34540fb72b22', metadata={'source': './test.txt'}, page_content='5.5 Challenges and Threats to the Amazon\\nDespite its vastness, the Amazon Jungle is extremely fragile. One of the greatest threats it faces is deforestation. Large areas of the jungle are cleared each year for cattle ranching, soybean farming, logging, and infrastructure projects such as roads and dams. Between 2000 and 2020, the Amazon lost millions of hectares of forest, accelerating biodiversity loss and contributing to global warming.\\n\\nMining is another destructive activity, both legal and illegal, often contaminating rivers with mercury and threatening indigenous territories. Fires, sometimes set deliberately to clear land, further damage large parts of the forest. Climate change adds another layer of danger, as rising temperatures and irregular rainfall can transform parts of the rainforest into savannah-like ecosystems—a process scientists fear could reach an irreversible \"tipping point.\"'),\n",
       " Document(id='7cdbf55e-594d-4ee3-9f2e-62108337cadc', metadata={'source': './test.txt'}, page_content='5.5 Challenges and Threats to the Amazon\\nDespite its vastness, the Amazon Jungle is extremely fragile. One of the greatest threats it faces is deforestation. Large areas of the jungle are cleared each year for cattle ranching, soybean farming, logging, and infrastructure projects such as roads and dams. Between 2000 and 2020, the Amazon lost millions of hectares of forest, accelerating biodiversity loss and contributing to global warming.\\n\\nMining is another destructive activity, both legal and illegal, often contaminating rivers with mercury and threatening indigenous territories. Fires, sometimes set deliberately to clear land, further damage large parts of the forest. Climate change adds another layer of danger, as rising temperatures and irregular rainfall can transform parts of the rainforest into savannah-like ecosystems—a process scientists fear could reach an irreversible \"tipping point.\"'),\n",
       " Document(id='28255b82-a568-4aa1-8e04-01e3da1b4fc3', metadata={'source': './test.txt'}, page_content='Conclusion\\nThe Amazon Jungle is more than just a rainforest—it is the heartbeat of the Earth’s ecological system, a home to countless species and human cultures, and a vital regulator of the planet’s climate. Its immense biodiversity, natural beauty, and cultural value make it one of the most precious regions on Earth. At the same time, it faces mounting challenges from deforestation, mining, climate change, and human exploitation. Preserving the Amazon is not just a regional issue but a global responsibility. Ensuring its survival will require determination, collaboration, and respect for the indigenous knowledge that has sustained it for millennia. Only then can this irreplaceable treasure continue to thrive as a source of life, balance, and inspiration for the entire world.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"query\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c334674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/porya/anaconda3/envs/LangChain/lib/python3.12/site-packages/langchain_core/indexing/api.py:385: UserWarning: Using SHA-1 for document hashing. SHA-1 is *not* collision-resistant; a motivated attacker can construct distinct inputs that map to the same fingerprint. If this matters in your threat model, switch to a stronger algorithm such as 'blake2b', 'sha256', or 'sha512' by specifying  `key_encoder` parameter in the the `index` or `aindex` function. \n",
      "  _warn_about_sha1()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 1: {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n",
      "Index attempt 2: {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n",
      "Index attempt 3: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "conncetion = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "collection_name = \"my_docs\"\n",
    "embeddings_model = OllamaEmbeddings(model='tinyllama')\n",
    "namespace=\"my_docs_namespace\"\n",
    "\n",
    "vectorstore = PGVector(embeddings=embeddings_model,\n",
    "                       collection_name=collection_name,\n",
    "                       connection=conncetion,\n",
    "                       use_jsonb=True)\n",
    "record_manager = SQLRecordManager(namespace, db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\")\n",
    "record_manager.create_schema()\n",
    "docs = [\n",
    "    Document(page_content='there are cats in the pond', metadata={\n",
    "             \"id\": 1, \"source\": \"cats.txt\"}),\n",
    "    Document(page_content='ducks are also found in the pond', metadata={\n",
    "             \"id\": 2, \"source\": \"ducks.txt\"}),\n",
    "]\n",
    "\n",
    "# Index the documents\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",  # prevent duplicate documents\n",
    "    source_id_key=\"source\",  # use the source field as the source_id\n",
    ")\n",
    "\n",
    "print(\"Index attempt 1:\", index_1)\n",
    "\n",
    "# second time you attempt to index, it will not add the documents again\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n",
    "print(\"Index attempt 2:\", index_2)\n",
    "\n",
    "# If we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.\n",
    "\n",
    "docs[0].page_content = \"I just modified this document!\"\n",
    "\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n",
    "print(\"Index attempt 3:\", index_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
